{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c45453b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import SimpleITK as sitk\n",
    "import os\n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from ipywidgets import interact, fixed\n",
    "import math\n",
    "import glob\n",
    "from scipy.interpolate import RegularGridInterpolator, interpn\n",
    "\n",
    "from skimage import measure\n",
    "\n",
    "import pylab as pl\n",
    "import trimesh\n",
    "from stl import mesh\n",
    "import re\n",
    "\n",
    "from scipy.spatial import KDTree\n",
    "import pyquaternion\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from test_utils import rigid_transform_3D, metrics\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ffe0dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare data set\n",
    "def centeroidnp(arr):\n",
    "    \"\"\"get the centroid of a point cloud\"\"\"\n",
    "    length = arr.shape[0]\n",
    "    sum_x = np.sum(arr[:, 0])\n",
    "    sum_y = np.sum(arr[:, 1])\n",
    "    sum_z = np.sum(arr[:, 2])\n",
    "    return math.ceil(sum_x/length), \\\n",
    "            math.ceil(sum_y/length), \\\n",
    "            math.ceil(sum_z/length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b9453018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_7D(source_pc, source_center, target_center):\n",
    "    \"\"\"create a 7D pointcloud as explained in Fu et al.\"\"\"\n",
    "    v_s = np.zeros((len(source_pc), 7))\n",
    "    for i in range(len(v_s)):\n",
    "        v_ss = source_center - source_pc[i,:3]\n",
    "        v_st = target_center - source_pc[i,:3]\n",
    "        v_s[i,:3] = v_ss \n",
    "        v_s[i,3:6] = v_st\n",
    "        v_s[i,6] = source_pc[i,3]\n",
    "    return v_s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b62cb489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_position_and_orientation(files_deform, files_regular):\n",
    "    \"\"\"\n",
    "    set the positioning and orientation of the deformed images so that they align\n",
    "    NOTE: this should be done in the begining when the initial deformed mhd files are created\n",
    "    \"\"\"\n",
    "    for d ,f in zip(files_deform, files_regular):\n",
    "        with open(d, 'a+') as deformed:\n",
    "            with open(f, 'r') as regular:\n",
    "                for r in regular.readlines():\n",
    "                    if \"Position\" in r: #or \"Orientation\" in r:\n",
    "                        deformed.write(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f94a9e3",
   "metadata": {},
   "source": [
    "# Make connections between vertebrae bodies and laminas with facets for XML scene in SOFA framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "292b0a84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def dist_pts(a, b):\n",
    "    return np.linalg.norm(a-b)\n",
    "\n",
    "def min_dist(points, p):\n",
    "    min_=min([dist_pts(a[:3],p) for a in points])\n",
    "    return [dist_pts(a[:3],p) for a in points].index(min_)\n",
    "\n",
    "def intersect2d(X, Y):\n",
    "        \"\"\"\n",
    "        Function to find intersection of two 2D arrays.\n",
    "        Returns index of rows in X that are common to Y.\n",
    "        \"\"\"\n",
    "        X = np.tile(X[:,:,None], (1, 1, Y.shape[0]) )\n",
    "        Y = np.swapaxes(Y[:,:,None], 0, 2)\n",
    "        Y = np.tile(Y, (X.shape[0], 1, 1))\n",
    "        eq = np.all(np.equal(X, Y), axis = 1)\n",
    "        eq = np.any(eq, axis = 1)\n",
    "        return np.nonzero(eq)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5aeeba32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_bbox(position):\n",
    "    \"\"\" \n",
    "    Gets the bounding box of the object defined by the given vertices.\n",
    "\n",
    "    Arguments\n",
    "    -----------\n",
    "    position : list\n",
    "    List with the coordinates of N points (position field of Sofa MechanicalObject).\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    xmin, xmax, ymin, ymax, zmin, zmax : floats\n",
    "    min and max coordinates of the object bounding box.\n",
    "    \"\"\"\n",
    "    points_array = np.asarray(position)\n",
    "    m = np.min(points_array, axis=0)\n",
    "    xmin, ymin, zmin = m[0], m[1], m[2]\n",
    "\n",
    "    m = np.max(points_array, axis=0)\n",
    "    xmax, ymax, zmax = m[0], m[1], m[2]\n",
    "\n",
    "    return xmin, xmax, ymin, ymax, zmin, zmax\n",
    "\n",
    "def get_indices_in_bbox( positions, bbox ):\n",
    "    \"\"\"\n",
    "    Get the indices of the points falling within the specified bounding box.\n",
    "\n",
    "    Arguments\n",
    "    ----------\n",
    "    positions (list):\n",
    "    N x 3 list of points coordinates.\n",
    "    bbox (list):\n",
    "    [xmin, ymin, zmin, xmax, ymax, zmax] extremes of the bounding box.\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    indices:\n",
    "    List of indices of points enclosed in the bbox.\n",
    "\n",
    "    \"\"\"\n",
    "    # bbox is in the format (xmin, ymin, zmin, xmax, ...)\n",
    "    assert len(bbox) == 6\n",
    "    indices = []\n",
    "    for i, x in enumerate( positions ):\n",
    "        if x[0] >= bbox[0] and x[0] <= bbox[3] and x[1] >= bbox[1] and x[1] <= bbox[4] and x[2] >= bbox[2] and x[2] <= bbox[5]:\n",
    "            indices.append( i )\n",
    "    return indices\n",
    "        \n",
    "\n",
    "def print_stiff_springs(vert1,vert2, bbox_v1_v2,bbox_v2_v1, s, d):\n",
    "    \"\"\"\n",
    "    vert1 and vert2: two adjecent vertebrae\n",
    "    bbox_v1_v2 and bbox_v2_v1 are the bounding boxes representing area where the \n",
    "    springs are found on the closer sides of two adjecent vertebrae\n",
    "    \"\"\"\n",
    "    idx1 = get_indices_in_bbox(vert1, bbox_v1_v2)[::5]\n",
    "    idx2 = get_indices_in_bbox(vert2, bbox_v2_v1)[::5]\n",
    "    print(\"SPRINGS: \")\n",
    "    np.random.shuffle(idx1)\n",
    "    np.random.shuffle(idx2)\n",
    "    print(min(len(idx1),len(idx2)))\n",
    "    print()\n",
    "    for i,j in zip(idx1,idx2):\n",
    "        print(\"{0} {1} {2} {3} {4}  \".format(i,j,s,d,dist_pts(vert1[i],vert2[j])), end='')\n",
    "        \n",
    "def print_positions(vert1, bbox_v1_t12):\n",
    "    \"\"\"\n",
    "    this function is used for seting the fixed points on L1 and L5 simulating\n",
    "    connection with T12 and S1 respectively\n",
    "    \"\"\"\n",
    "    \n",
    "    idx1 = get_indices_in_bbox(vert1, bbox_v1_t12)[::5]\n",
    "    print(\"POSITIONS:\")\n",
    "    for i in idx1:\n",
    "        print(\"{0} {1} {2}  \".format(vert1[i][0],vert1[i][1],vert1[i][2]), end='')\n",
    "    print(\"Indexes for fixed constraint\")\n",
    "    for i,_ in enumerate(idx1):\n",
    "        print(i, end=\" \")\n",
    "    print(\"SPRINGS: \")\n",
    "    for i,j in enumerate(idx1):\n",
    "        print(\"{0} {1} {2} {3} {4}  \".format(i,j,1000,10,0.00001), end='')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9647b6ba",
   "metadata": {},
   "source": [
    "## uncomment the block below to print the connections and vertices for the springs in SOFA framework change paths and bounding boxes accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "675d48bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPRINGS: \n",
      "461\n",
      "\n",
      "1697 16984 8000 500 0.013638694988890976  2134 11629 8000 500 0.02264568835341508  5526 16729 8000 500 0.018569577297289253  1007 15978 8000 500 0.0182612486155794  2493 15973 8000 500 0.02774905405594936  396 16651 8000 500 0.013447646671444042  4828 16834 8000 500 0.009486727623369396  5476 12169 8000 500 0.035539713012347186  947 16053 8000 500 0.012049116191654897  1417 14159 8000 500 0.016368396408933873  1437 12570 8000 500 0.023608964420321375  4868 16681 8000 500 0.01606334961955319  4783 16023 8000 500 0.03392297747839951  4863 11311 8000 500 0.02735754376767036  1198 11351 8000 500 0.020257127165518796  598 15444 8000 500 0.021953642089639718  2069 16884 8000 500 0.015817812775475643  5905 17029 8000 500 0.010489957149578838  603 11577 8000 500 0.015822237547199205  4838 12015 8000 500 0.04204434802681566  1233 16696 8000 500 0.02856557370332338  5171 17019 8000 500 0.007063653516417697  5840 14560 8000 500 0.013964598132420415  5934 12907 8000 500 0.024015145242117522  6411 13417 8000 500 0.01952744737542519  5511 12502 8000 500 0.02206491313012586  789 13352 8000 500 0.017464317376868748  804 16999 8000 500 0.01615763599664257  1906 15484 8000 500 0.022759393687003162  1362 16400 8000 500 0.010616476884541294  5516 16008 8000 500 0.02184119962364704  799 13238 8000 500 0.03534825018922437  3044 11560 8000 500 0.0355672040087494  528 13729 8000 500 0.003721988850063914  5885 16904 8000 500 0.008306864691326092  1921 13337 8000 500 0.01476490436812919  1133 12812 8000 500 0.017931580840517092  4221 13387 8000 500 0.014297356433970598  1208 12484 8000 500 0.01321697397288804  4768 16048 8000 500 0.03586414924405708  5855 13644 8000 500 0.01884181522571539  3604 16345 8000 500 0.018177986451749808  6388 11261 8000 500 0.03274220489582214  1542 16839 8000 500 0.017262885071736987  4291 16063 8000 500 0.015684323415436175  5561 16959 8000 500 0.012697684867722945  3654 16385 8000 500 0.028600489531474805  3619 11282 8000 500 0.016901561804756382  1422 12892 8000 500 0.016093663380349437  1582 12980 8000 500 0.024514363157137106  2488 13232 8000 500 0.015032271983968348  2483 13759 8000 500 0.016266689921431467  1382 14655 8000 500 0.002459105731765084  5919 12174 8000 500 0.023601440655180345  2533 13263 8000 500 0.0332263630420183  5461 11235 8000 500 0.03475944765096248  194 16641 8000 500 0.013119344533931564  957 11245 8000 500 0.010695063881997134  1243 11269 8000 500 0.016576585927144342  2079 15399 8000 500 0.009808628905203821  814 16711 8000 500 0.029589714446070614  2049 12077 8000 500 0.017469321709785986  4803 16013 8000 500 0.009013501040106441  199 16764 8000 500 0.03157410966282343  1547 14069 8000 500 0.03283330627579257  1002 13669 8000 500 0.0165312189810673  952 13754 8000 500 0.03697805296388658  1717 16631 8000 500 0.03353063078738604  2094 16671 8000 500 0.025020975220802242  5151 11619 8000 500 0.032039681661964116  446 13664 8000 500 0.030606326159799045  4271 13679 8000 500 0.01862866610898376  3024 17009 8000 500 0.0036195581222021904  366 16058 8000 500 0.010419481342178226  6281 14585 8000 500 0.013648120069811814  5141 13015 8000 500 0.02269007693772764  3069 12851 8000 500 0.027874145744757793  5536 14219 8000 500 0.03271180828080283  834 11331 8000 500 0.010774358052338902  239 11654 8000 500 0.027981193702199335  1901 14565 8000 500 0.015538390889664237  1432 16666 8000 500 0.020798586514472545  2074 15988 8000 500 0.005916924961498142  1067 16596 8000 500 0.02600567632268002  849 12444 8000 500 0.02279107722333458  1188 15409 8000 500 0.01417674155086423  1238 17069 8000 500 0.029715605344666968  1387 11764 8000 500 0.02251673157898365  5211 12138 8000 500 0.029431058441721065  5131 15439 8000 500 0.016561292250304626  1148 13719 8000 500 0.0055037442709486425  5491 11674 8000 500 0.035632967894914394  5471 13382 8000 500 0.027544640150127225  1881 12595 8000 500 0.011165804558561828  844 13362 8000 500 0.024187682836518256  1258 16395 8000 500 0.026223981410152045  4311 11707 8000 500 0.030488312531197933  6329 11131 8000 500 0.0344855216141499  5466 12960 8000 500 0.04330853496714013  1886 16759 8000 500 0.009244955435263045  829 13297 8000 500 0.024207486061960253  749 12116 8000 500 0.017717595532125704  613 11165 8000 500 0.03433268415082048  5880 12565 8000 500 0.032566166507588826  426 12927 8000 500 0.02378249778723843  623 16716 8000 500 0.01127911348466714  361 12048 8000 500 0.006429556827651475  2149 11202 8000 500 0.02501070172945974  1846 11297 8000 500 0.034403639356905255  1143 11600 8000 500 0.016928909635295464  2558 11151 8000 500 0.02531179963969373  326 16425 8000 500 0.017108395629047157  2568 13367 8000 500 0.031654061215584964  2578 14084 8000 500 0.02845897399766902  744 12856 8000 500 0.021566176792375595  1916 12454 8000 500 0.017903418695880396  578 12590 8000 500 0.0301079923110127  1767 15419 8000 500 0.021493394357336857  1537 16018 8000 500 0.026044177871455253  759 13694 8000 500 0.007218670306919403  1687 14164 8000 500 0.01170480247590707  1223 16330 8000 500 0.025747566894757268  628 16606 8000 500 0.020977921751212637  421 16989 8000 500 0.01907423395578445  1722 16809 8000 500 0.021328553654666776  739 13357 8000 500 0.006086771840639345  1762 12985 8000 500 0.030869648540273336  5865 13377 8000 500 0.01817873431897832  1062 14625 8000 500 0.021657238997619246  1213 16924 8000 500 0.012859362386992607  5201 13774 8000 500 0.030033248259220977  784 12060 8000 500 0.015082678840312152  1168 16819 8000 500 0.007855647713587956  4246 13317 8000 500 0.0249280805719173  2119 16068 8000 500 0.03021268609375869  6202 12557 8000 500 0.0212086114821315  1552 13302 8000 500 0.016373160995971435  1742 12099 8000 500 0.02389531336894328  4231 14214 8000 500 0.029898113669594602  1562 14550 8000 500 0.029256643707028324  5910 11732 8000 500 0.030527063262619936  1203 16799 8000 500 0.00992528090282586  5126 17024 8000 500 0.023432737804191802  5541 14154 8000 500 0.03724133726116717  1532 11975 8000 500 0.01884877717519098  6361 17094 8000 500 0.03536230197540876  1876 14199 8000 500 0.023574316554250307  3049 14114 8000 500 0.02720258813863124  5481 11228 8000 500 0.021963679131693756  962 11684 8000 500 0.007892984289861472  4321 13699 8000 500 0.025807634548714455  3639 13000 8000 500 0.026133411583641363  6210 16944 8000 500 0.034259136022380954  538 12038 8000 500 0.00989147112415539  3614 12523 8000 500 0.030016162170404124  583 13332 8000 500 0.01361701880001639  1911 16335 8000 500 0.02918881979457202  4823 13654 8000 500 0.018159636324552326  5940 16450 8000 500 0.028097231198109182  1587 16824 8000 500 0.007337928931244856  2543 13283 8000 500 0.021789437611833856  376 12002 8000 500 0.01218504004917504  2523 11743 8000 500 0.018659710379317237  588 12164 8000 500 0.012657440161422832  5521 15464 8000 500 0.03765636733674663  937 15449 8000 500 0.030080824314503086  972 11624 8000 500 0.026293900452386307  869 12069 8000 500 0.01641752724986319  436 16003 8000 500 0.027598934780168595  100 16954 8000 500 0.03227886616658027  1347 16430 8000 500 0.025397834573049723  1352 11754 8000 500 0.011357200403268396  1158 11712 8000 500 0.0075370021228602715  5556 12518 8000 500 0.01890530061755169  1727 17059 8000 500 0.017434425743338942  5121 15489 8000 500 0.025468749498159516  1218 16909 8000 500 0.01729957227794954  593 15384 8000 500 0.01812005521514766  1502 13689 8000 500 0.030309424293443785  411 13288 8000 500 0.014700108877147827  3079 16626 8000 500 0.02296582680854315  4793 16038 8000 500 0.01470629800459652  5870 13268 8000 500 0.02641614641540283  2099 11646 8000 500 0.02407430978117547  4256 11962 8000 500 0.021120397747201647  982 14630 8000 500 0.005114058216328791  401 13779 8000 500 0.014129062283109936  5106 15998 8000 500 0.012836790153305463  819 12922 8000 500 0.022020999091776  1597 12580 8000 500 0.01479817559701195  839 13347 8000 500 0.012104924658997245  351 16894 8000 500 0.011565950069060475  316 16794 8000 500 0.02288272713205311  5486 16979 8000 500 0.009510057886259164  2144 16784 8000 500 0.013917198029775974  618 17054 8000 500 0.02015051366590934  6352 14675 8000 500 0.01370383891469832  987 12479 8000 500 0.04289389701344472  3059 14204 8000 500 0.025339755346095998  2109 16440 8000 500 0.03403482335784924  1402 11212 8000 500 0.01413934938390023  2054 12878 8000 500 0.01687392073585744  1926 14615 8000 500 0.014847397111951995  3084 16691 8000 500 0.01643258960115537  3584 14595 8000 500 0.02325966467944024  5860 16874 8000 500 0.02365242484397743  3679 16869 8000 500 0.019391678653484328  764 12094 8000 500 0.010077063113824385  3089 17074 8000 500 0.024696218354233912  648 17034 8000 500 0.023761043769161317  1412 14139 8000 500 0.018512946848084452  2994 12449 8000 500 0.014346511472828512  4758 14079 8000 500 0.03199295549023256  1522 11689 8000 500 0.012762428900487559  4286 12953 8000 500 0.02147063112719326  5456 16033 8000 500 0.03238946729169839  5531 15514 8000 500 0.014178011179287455  824 12585 8000 500 0.028145997957080886  1327 12469 8000 500 0.03438735787233442  1163 17084 8000 500 0.016145191265513094  4833 12897 8000 500 0.027146970383451624  3094 16899 8000 500 0.016603867049576143  6316 16789 8000 500 0.04081404661388035  321 15424 8000 500 0.027910750634836026  3589 15948 8000 500 0.029614226327898552  1337 16611 8000 500 0.006469095840996645  2538 14690 8000 500 0.03755497837837216  2583 16994 8000 500 0.025805100290446484  2129 13684 8000 500 0.027006614023235124  4848 11191 8000 500 0.03728354061781151  5161 16405 8000 500 0.018810029266324926  416 15474 8000 500 0.03570743901486074  1517 15983 8000 500 0.03233985777643434  1037 12932 8000 500 0.01824229155013152  4798 13764 8000 500 0.030609541012566674  2573 16360 8000 500 0.031007966580219377  1342 17044 8000 500 0.013982603512937076  3034 14194 8000 500 0.0217672047125946  779 11699 8000 500 0.02466561942137273  608 11984 8000 500 0.024272066269685408  1253 12474 8000 500 0.015025751262416117  3644 12832 8000 500 0.022132130511995432  356 12420 8000 500 0.0188807097589047  189 16410 8000 500 0.031064416958958042  336 16601 8000 500 0.013932523138326394  4261 17004 8000 500 0.01659208247930321  1672 12121 8000 500 0.011023266348954857  5216 12965 8000 500 0.02453016920039483  1866 12082 8000 500 0.009303074814275116  1871 16621 8000 500 0.006730616687941756  1712 16854 8000 500 0.017551501388770124  90 12410 8000 500 0.009495325218232398  5546 16455 8000 500 0.017514314174411724  3054 11292 8000 500 0.02761150486663122  5850 13744 8000 500 0.02228475714473911  1012 13769 8000 500 0.01712446206454383  2104 16929 8000 500 0.011475155815935579  1747 16949 8000 500 0.02793940588129965  4241 12033 8000 500 0.02210007242069581  1512 12133 8000 500 0.01686749539795402  4778 12489 8000 500 0.03991369690970758  3659 16370 8000 500 0.03479609749670211  4281 12942 8000 500 0.022986717925793588  5496 16701 8000 500 0.0306474012111957  184 11614 8000 500 0.023823584973718812  573 16749 8000 500 0.014457980529797371  234 13010 8000 500 0.004231359237880895  1836 11666 8000 500 0.01272954048660046  2518 15958 8000 500 0.017135973606422262  5196 13412 8000 500 0.014877809012082386  5566 11759 8000 500 0.03271779945228591  4251 15404 8000 500 0.024463891584128657  1592 12028 8000 500 0.04036425647773036  4316 14184 8000 500 0.031434204316317604  992 14099 8000 500 0.02052508711309161  5501 16375 8000 500 0.017773440888021654  6245 12912 8000 500 0.03893149857685933  1392 16390 8000 500 0.00754890727191689  5571 13312 8000 500 0.02958097363171132  5895 14650 8000 500 0.03547912627165444  729 15469 8000 500 0.024366132253601513  1027 14580 8000 500 0.022003027087198708  1397 14680 8000 500 0.009079273153727675  1682 13734 8000 500 0.011201392815181501  3599 12382 8000 500 0.015573862751417838  441 12464 8000 500 0.0170880771721104  4211 14134 8000 500 0.013191285039752572  5146 17039 8000 500 0.004999200036005765  4808 16754 8000 500 0.009915462722435082  864 12020 8000 500 0.023017888717256428  1052 16420 8000 500 0.026036535887095293  1567 14169 8000 500 0.01331071752386023  1891 16804 8000 500 0.013390160603965862  1737 11679 8000 500 0.020631955589327945  3689 15504 8000 500 0.024108861462126353  643 13253 8000 500 0.025839334376101895  1183 16864 8000 500 0.027590092442759225  2548 16636 8000 500 0.022926997208531277  1707 12159 8000 500 0.014011466768329447  4306 15414 8000 500 0.01719540611442487  3064 11306 8000 500 0.019025351545766508  2513 15963 8000 500 0.030211904954835247  1427 11287 8000 500 0.012205900253565887  1248 14665 8000 500 0.020879655193513154  967 12842 8000 500 0.014212177911917664  2563 16616 8000 500 0.014093629802148193  1572 12391 8000 500 0.0330004848600744  5111 16073 8000 500 0.021688042811650862  3674 15389 8000 500 0.022245997200395416  3074 15968 8000 500 0.02334497807752239  1602 11992 8000 500 0.03017419413074689  4788 14089 8000 500 0.025882905390237777  3684 16934 8000 500 0.021562977554132008  3009 14144 8000 500 0.025183470789388814  543 16969 8000 500 0.025405007400116995  2059 12861 8000 500 0.019301119164442247  1856 15499 8000 500 0.03252789573581421  3629 14620 8000 500 0.01750635316106699  4843 12089 8000 500 0.03328501151329229  311 13307 8000 500 0.007335337854523136  1732 16774 8000 500 0.010446071079597335  1022 16844 8000 500 0.02878477377017231  204 14174 8000 500 0.021449018648879967  4773 11636 8000 500 0.022741618258162715  558 16974 8000 500 0.024951376735563125  533 12990 8000 500 0.010462121746567483  5181 17049 8000 500 0.022581124883406512  2508 16656 8000 500 0.029044951385740002  1702 14600 8000 500 0.013763865425090427  4326 15479 8000 500 0.028830109278322195  6226 11240 8000 500 0.023807797063147205  1757 17109 8000 500 0.025066176433592734  5451 11641 8000 500 0.029040585246168843  4763 16365 8000 500 0.011601310313925764  4858 16829 8000 500 0.016940814649833096  2064 11659 8000 500 0.014435553366601513  3669 12600 8000 500 0.014776237714655228  1228 17099 8000 500 0.016128248540991662  5890 12947 8000 500 0.027891045004445423  219 16814 8000 500 0.013581428533110933  1057 14660 8000 500 0.030629090763520873  3624 13248 8000 500 0.01737310540577015  563 13649 8000 500 0.030467313649220867  5820 12154 8000 500 0.02439323678809357  5830 13659 8000 500 0.036261610568202846  341 16043 8000 500 0.020263346243895645  5924 11159 8000 500 0.02843721507110006  997 12995 8000 500 0.03615485031085042  2498 14124 8000 500 0.02439887704383135  2588 13739 8000 500 0.023600440704359738  2503 16340 8000 500 0.010412223633787366  5875 16646 8000 500 0.010030473617930498  658 12143 8000 500 0.025290931200728832  1032 16779 8000 500 0.01359923530938414  3019 12970 8000 500 0.029576679858293757  2084 16964 8000 500 0.014304908283522818  5825 11181 8000 500 0.021609127271595197  2553 17014 8000 500 0.026558448768706354  1193 11997 8000 500 0.01606488098306362  1931 13704 8000 500 0.03162968212676189  1407 16355 8000 500 0.017805167817237757  371 16350 8000 500 0.013153797968647681  1017 11207 8000 500 0.028509317792609477  859 16415 8000 500 0.02100317121293828  3004 15459 8000 500 0.017322909715171975  4296 13327 8000 500 0.017004740544918646  5166 13392 8000 500 0.02349866360966088  1138 12459 8000 500 0.03642079627081211  1677 16435 8000 500 0.007807893505933581  1752 14189 8000 500 0.007800333390310973  638 16919 8000 500 0.01791728776907933  2124 15509 8000 500 0.035217881835794704  1896 16859 8000 500 0.03008328441177924  568 11569 8000 500 0.030642767515353435  854 13020 8000 500 0.021234603632750003  769 14179 8000 500 0.029062814746682743  4236 12053 8000 500 0.03367500558277607  1507 16676 8000 500 0.032284751834263804  734 12513 8000 500 0.003147125831612089  4813 16380 8000 500 0.021743072482977206  2999 14610 8000 500 0.012186927463474934  754 12827 8000 500 0.04273611120586428  1557 15379 8000 500 0.030882014044423988  5206 12917 8000 500 0.02437447828036532  386 14209 8000 500 0.03021577074045936  5835 12818 8000 500 0.021656961952222192  5156 11256 8000 500 0.03466409094437643  214 15494 8000 500 0.008071877167053522  5191 13372 8000 500 0.015263944477100265  1263 17079 8000 500 0.02109787669411308  5186 14605 8000 500 0.021609599520583455  5945 16849 8000 500 0.025062705380704615  4301 14149 8000 500 0.024484305197411675  431 14645 8000 500 0.008777197787448997  2139 16686 8000 500 0.03734701060325978  548 16889 8000 500 0.013006506102716446  110 12975 8000 500 0.0042250681651305995  942 12866 8000 500 0.03577111126314083  1527 15454 8000 500 0.0024896588119659832  1577 13674 8000 500 0.03246316683566162  1377 14555 8000 500 0.028637025002608067  2089 14064 8000 500 0.013779941634128946  2114 11220 8000 500 0.016799404496588573  809 14109 8000 500 0.017799078106463827  95 15429 8000 500 0.009950577922914816  3634 12552 8000 500 0.015827798362374964  3029 14057 8000 500 0.015410450721507152  6311 13243 8000 500 0.04402912651234406  5176 11717 8000 500 0.038587485030771314  5116 14119 8000 500 0.012694613070117574  105 11694 8000 500 0.0050144990776746215  3609 16028 8000 500 0.015285718857809712  346 13322 8000 500 0.03159211928630303  1861 16734 8000 500 0.011689876004474995  6334 13709 8000 500 0.03368820566607844  3014 13402 8000 500 0.01898458324535991  4266 11317 8000 500 0.02089991389934419  1153 12104 8000 500 0.04141196929632786  4216 11583 8000 500 0.017943321638983108  406 12575 8000 500 0.013191891486818697  1173 11274 8000 500 0.007369070565546246  4818 14670 8000 500 0.03936087398673966  6293 14685 8000 500 0.016075173436078383  381 16939 8000 500 0.018679518221838602  1332 14074 8000 500 0.025319952240081336  391 11142 8000 500 0.02612009190259485  633 14590 8000 500 0.01671629148465651  209 16445 8000 500 0.034314900568120525  3664 14640 8000 500 0.029258024394001724  5506 15434 8000 500 0.021290542524792542  5845 11196 8000 500 0.04176191088779344  331 16739 8000 500 0.01432023746311492  794 13639 8000 500 0.017689296226814696  774 14094 8000 500 0.011768754880615025  4853 16744 8000 500 0.023348773008447374  3594 13724 8000 500 0.03259489517455149  1841 12043 8000 500 0.0336817042472616  2528 11595 8000 500 0.030162665681268942  1607 13005 8000 500 0.014902362262406576  1357 13342 8000 500 0.009105558796691145  5950 12497 8000 500 0.022873504344546783  4226 12937 8000 500 0.014463374467944884  4276 14129 8000 500 0.015654941438408506  553 12430 8000 500 0.011489873846130785  5900 11605 8000 500 0.029349841583899576  224 17104 8000 500 0.02978471421719537  3039 17064 8000 500 0.014361518060428026  1372 14635 8000 500 0.03014966635702624  1851 13273 8000 500 0.018880476715379842  3649 12902 8000 500 0.036465989647889725  1178 16325 8000 500 0.02450738626292084  1692 15993 8000 500 0.030144837053797457  1367 14575 8000 500 0.013509211355219805  5551 14570 8000 500 0.020804383947620277  1047 11340 8000 500 0.02085641390555911  653 13714 8000 500 0.02646003025319509  6380 13397 8000 500 0.04726564081275105  1042 16769 8000 500 0.023764014833356745  977 14104 8000 500 0.025949474002376245  229 12536 8000 500 0.02534646328385877  5136 16706 8000 500 0.029450181680254532  "
     ]
    }
   ],
   "source": [
    "vert1 = np.loadtxt(\"/Users/janelameski/Desktop/jane/Thesis/prethesis/DataJane/Spine8/v1.txt\")[:,:3]\n",
    "vert2 = np.loadtxt(\"/Users/janelameski/Desktop/jane/Thesis/prethesis/DataJane/Spine8/v2.txt\")[:,:3]\n",
    "# vert3 = np.loadtxt(\"/Users/janelameski/Desktop/jane/Thesis/prethesis/DataJane/Spine10/v3.txt\")[:,:3]\n",
    "# vert4 = np.loadtxt(\"/Users/janelameski/Desktop/jane/Thesis/prethesis/DataJane/Spine10/v4.txt\")[:,:3]\n",
    "# vert5 = np.loadtxt(\"/Users/janelameski/Desktop/jane/Thesis/prethesis/DataJane/Spine10/v5.txt\")[:,:3] \n",
    "              \n",
    "bbox_v1_t12 = [-0.135, 0.189, -0.26, -0.127, 0.258, -0.219]\n",
    "bbox_v1_v2 = [-0.109, 0.189, -0.26, -0.09, 0.258, -0.219]\n",
    "bbox_v2_v1 = [-0.109, 0.189, -0.25, -0.09, 0.258, -0.219]\n",
    "bbox_v2_v3 = [-0.076, 0.189, -0.25, -0.02, 0.258, -0.215]\n",
    "bbox_v3_v2 = [-0.076, 0.202, -0.25, -0.061, 0.258, -0.215]\n",
    "bbox_v3_v4 = [-0.042, 0.202, -0.25, -0.03, 0.258, -0.215]\n",
    "bbox_v4_v3 = [-0.041, 0.202, -0.247, -0.0265, 0.258, -0.215]\n",
    "bbox_v4_v5 = [-0.009, 0.195, -0.247, 0.01, 0.258, -0.215]\n",
    "bbox_v5_v4 = [-0.009, 0.195, -0.247, 0.01, 0.258, -0.215]\n",
    "bbox_v5_s1 = [0.018, 0.195, -0.27, 0.04, 0.258, -0.215]\n",
    "\n",
    "bbox_bone_v1_v2 = [-0.11, 0.195, -0.28, -0.088, 0.25, -0.255]\n",
    "bbox_bone_v2_v1 = [-0.13, 0.195, -0.275, -0.088, 0.25, -0.255]\n",
    "bbox_bone_v2_v3 = [-0.07, 0.195, -0.275, -0.06, 0.25, -0.25]\n",
    "bbox_bone_v3_v2 = [-0.08, 0.195, -0.27, -0.06, 0.25, -0.25]\n",
    "bbox_bone_v3_v4 = [-0.054, 0.19, -0.27, -0.027, 0.24, -0.25]\n",
    "bbox_bone_v4_v3 = [-0.06, 0.19, -0.27, -0.027, 0.24, -0.25]\n",
    "bbox_bone_v4_v5 = [-0.02, 0.18, -0.285, 0, 0.24, -0.26]\n",
    "bbox_bone_v5_v4 = [-0.05, 0.18, -0.285, 0, 0.24, -0.26]\n",
    "\n",
    "# print_positions(vert1, bbox_v1_t12)\n",
    "# print()\n",
    "# print_positions(vert5, bbox_v5_s1)\n",
    "# print()\n",
    "# # format(i,j,500,3,dist_pts(vert1[i],vert2[j])), end='')\n",
    "# print_stiff_springs(vert1, vert2, bbox_v1_v2, bbox_v2_v1,500,3)\n",
    "# print()\n",
    "# print_stiff_springs(vert2, vert3, bbox_v2_v3, bbox_v3_v2,500,3)\n",
    "# print()\n",
    "# print_stiff_springs(vert3, vert4, bbox_v3_v4, bbox_v4_v3,500,3)\n",
    "# print()\n",
    "# print_stiff_springs(vert4, vert5, bbox_v4_v5, bbox_v5_v4,500,3)\n",
    "# # format(i,j,8000,500,dist_pts(vert1[i],vert2[j])), end='')\n",
    "# print()\n",
    "# print()\n",
    "print_stiff_springs(vert1, vert2, bbox_bone_v1_v2, bbox_bone_v2_v1,8000,500)\n",
    "# print()\n",
    "# print_stiff_springs(vert2, vert3, bbox_bone_v2_v3, bbox_bone_v3_v2,8000,500)\n",
    "# print()\n",
    "# print_stiff_springs(vert3, vert4, bbox_bone_v3_v4, bbox_bone_v4_v3,8000,500)\n",
    "# print()\n",
    "# print_stiff_springs(vert4, vert5, bbox_bone_v4_v5, bbox_bone_v5_v4,8000,500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9df29f",
   "metadata": {},
   "source": [
    "## Take points for Biomechanical constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3debf845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_point_idx(point_cloud, input_point):\n",
    "    \"\"\"\n",
    "    Returns the index closest point to <input_point> in the input <point_cloud>\n",
    "    \"\"\"\n",
    "    idx = np.array([np.linalg.norm(x+y+z) for (x,y,z) in np.abs(point_cloud[:,:3]-input_point[:3])]).argmin()\n",
    "    return int(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12f57699",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bboxes(values):\n",
    "    \"\"\"\n",
    "    Returns the bounding boxes from a file created manualy in ImFusion\n",
    "    \"\"\"\n",
    "    return [np.min(values[:,0]), np.min(values[:,1]) ,np.min(values[:,2]),\n",
    "      np.max(values[:,0]), np.max(values[:,1]),np.max(values[:,2])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "086ff9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vert1 = np.loadtxt(\"DataJane/Spine10/v1.txt\")[:,:3]\n",
    "# vert2 = np.loadtxt(\"DataJane/Spine10/v2.txt\")[:,:3]\n",
    "# vert3 = np.loadtxt(\"DataJane/Spine10/v3.txt\")[:,:3]\n",
    "# vert4 = np.loadtxt(\"DataJane/Spine10/v4.txt\")[:,:3]\n",
    "# vert5 = np.loadtxt(\"DataJane/Spine10/v5.txt\")[:,:3]\n",
    "\n",
    "# verts = [vert1,vert2,vert3,vert4,vert5]\n",
    "# #bounding boxes of the vertebrae, produced manually and saved in the above cell for \n",
    "# #every spine separately\n",
    "# bbox_v1_t12 = [-0.0239, 0.2275, -0.0591, 0.0629, 0.2764, -0.0442]\n",
    "# bbox_v1_v2 = [-0.0239, 0.2275, -0.08, 0.0629, 0.272, -0.062]\n",
    "# bbox_v2_v1 = [-0.0239, 0.2275, -0.087, 0.0629, 0.266, -0.062]\n",
    "# bbox_v2_v3 = [-0.0239, 0.2265, -0.11, 0.0629, 0.262, -0.09]\n",
    "# bbox_v3_v2 = [-0.0239, 0.2, -0.115, 0.0629, 0.253, -0.09]\n",
    "# bbox_v3_v4 = [-0.0239, 0.2, -0.14, 0.0629, 0.248, -0.122]\n",
    "# bbox_v4_v3 = [-0.0239, 0.2, -0.15, 0.0629, 0.245, -0.122]\n",
    "# bbox_v4_v5 = [-0.0239, 0.2, -0.18, 0.0629, 0.245, -0.16]\n",
    "# bbox_v5_v4 = [-0.0239, 0.19, -0.187, 0.0629, 0.24, -0.16]\n",
    "# bbox_v5_s1 = [-0.0239, 0.19, -0.214, 0.0629, 0.24, -0.195]\n",
    "\n",
    "# bboxes = [bbox_v1_t12, bbox_v1_v2,\n",
    "#           bbox_v2_v1, bbox_v2_v3, \n",
    "#           bbox_v3_v2, bbox_v3_v4,\n",
    "#           bbox_v4_v3, bbox_v4_v5,\n",
    "#           bbox_v5_v4, bbox_v5_s1]\n",
    "\n",
    "# len_v = 0\n",
    "# with open(\"Spine10_biomechanical.txt\", \"w+\") as file:\n",
    "#     indices1 = get_indices_in_bbox(verts[0], bboxes[1])\n",
    "#     indices2 = get_indices_in_bbox(verts[1], bboxes[2])\n",
    "#     indices3 = get_indices_in_bbox(verts[1], bboxes[3])\n",
    "#     indices4 = get_indices_in_bbox(verts[2], bboxes[4])\n",
    "#     indices5 = get_indices_in_bbox(verts[2], bboxes[5])\n",
    "#     indices6 = get_indices_in_bbox(verts[3], bboxes[6])\n",
    "#     indices7 = get_indices_in_bbox(verts[3], bboxes[7])\n",
    "#     indices8 = get_indices_in_bbox(verts[4], bboxes[8])\n",
    "\n",
    "    \n",
    "#     a1 = find_nearest_vector(verts[0],np.mean(verts[0][indices1], axis=0))\n",
    "#     a2 = find_nearest_vector(verts[1],np.mean(verts[1][indices2], axis=0))\n",
    "#     a3 = find_nearest_vector(verts[1],np.mean(verts[1][indices3], axis=0))\n",
    "#     a4 = find_nearest_vector(verts[2],np.mean(verts[2][indices4], axis=0))\n",
    "#     a5 = find_nearest_vector(verts[2],np.mean(verts[2][indices5], axis=0))\n",
    "#     a6 = find_nearest_vector(verts[3],np.mean(verts[3][indices6], axis=0))\n",
    "#     a7 = find_nearest_vector(verts[3],np.mean(verts[3][indices7], axis=0))\n",
    "#     a8 = find_nearest_vector(verts[4],np.mean(verts[4][indices8], axis=0))\n",
    "    \n",
    "#     file.write(\"{0} {1} {2} {3} {4} {5} {6} {7}\".format(a1,a2,a3,a4,a5,a6,a7,a8))\n",
    "    #SANITY CHECK\n",
    "#     file.write(\"{0} {1} {2}\\n{3} {4} {5}\\n{6} {7} {8}\\n{9} {10} {11}\\n{12} {13} {14}\\n{15} {16} {17}\\n{18} {19} {20}\\n{21} {22} {23}\\n\"\n",
    "#                                 .format(verts[0][a1][0],\n",
    "#                                         verts[0][a1][1],\n",
    "#                                         verts[0][a1][2],\n",
    "#                                         verts[1][a2][0],\n",
    "#                                         verts[1][a2][1],\n",
    "#                                         verts[1][a2][2],\n",
    "#                                          verts[1][a3][0],\n",
    "#                                         verts[1][a3][1],\n",
    "#                                         verts[1][a3][2],\n",
    "#                                         verts[2][a4][0],\n",
    "#                                         verts[2][a4][1],\n",
    "#                                         verts[2][a4][2],\n",
    "#                                          verts[2][a5][0],\n",
    "#                                         verts[2][a5][1],\n",
    "#                                         verts[2][a5][2],\n",
    "#                                         verts[3][a6][0],\n",
    "#                                         verts[3][a6][1],\n",
    "#                                         verts[3][a6][2],\n",
    "#                                          verts[3][a7][0],\n",
    "#                                         verts[3][a7][1],\n",
    "#                                         verts[3][a7][2],\n",
    "#                                         verts[4][a8][0],\n",
    "#                                         verts[4][a8][1],\n",
    "#                                         verts[4][a8][2],\n",
    "#                                                          ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d74e22",
   "metadata": {},
   "source": [
    "## 2 Creating the dataset\n",
    "The following scripts describe how to reorder and preprocess the .vtu data, in output from the sofa framework to a\n",
    "data format compatible with the network loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc733d9d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 2. 1 Reordering Files generated from sofa:\n",
    "The output files from sofa are reordered such that DbFolder (i.e. where the\n",
    "dataset is saved) is organized as follows: DbFolder/Spine<i>/timestamp<t> with <i> being the spine id and <t> the\n",
    "simulation timestamp. Each of such folder contains 5 files, one corresponding to each vertebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a03db12d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from shutil import copy2\n",
    "\n",
    "def extract_spine_id(filename):\n",
    "    \"\"\"\n",
    "    Given a file, it extracts the id of the spine.\n",
    "\n",
    "    Example 1.\n",
    "\n",
    "    .. code-block:: console\n",
    "    >> filename = <spine_folder>\\sspine1_vert1_1_0.txt\n",
    "    >> extract_spine_id(filename)\n",
    "    spine_1\n",
    "\n",
    "    Example 2:\n",
    "    >> filename = spine1_vert1_0.txt\n",
    "    >> extract_spine_id(filename)\n",
    "    spine_1\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    filename = os.path.split(filename)[-1]\n",
    "\n",
    "    return filename.split(\"_\")[0]\n",
    "\n",
    "def extract_vertebra_id(filename):\n",
    "    \"\"\"\n",
    "    Given a file, it extracts the id of the vertebra\n",
    "\n",
    "    Example 1.\n",
    "\n",
    "    .. code-block:: console\n",
    "    >> filename = <spine_folder>\\sspine1_vert1_1_0.txt\n",
    "    >> extract_vertebra_id(filename)\n",
    "    vert1\n",
    "\n",
    "    Example 2:\n",
    "    >> filename = spine1_vert1_0.txt\n",
    "    >> extract_vertebra_id(filename)\n",
    "    vert1\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    filename = os.path.split(filename)[-1]\n",
    "    return filename.split(\"_\")[1][0:5]\n",
    "\n",
    "\n",
    "def extract_timestamp_id(filename):\n",
    "    \"\"\"\n",
    "    Given a file, it extracts the id of the timestamp\n",
    "\n",
    "    Example 1.\n",
    "\n",
    "    .. code-block:: console\n",
    "    >> filename = <spine_folder>\\sspine1_vert1_1_0.txt\n",
    "    >> extract_timestamp_id(filename)\n",
    "    1_0\n",
    "\n",
    "    Example 2:\n",
    "    >> filename = spine1_vert1_1_0.txt\n",
    "    >> extract_timestamp_id(filename)\n",
    "    1_0\n",
    "\n",
    "    \"\"\"\n",
    "    filename = os.path.split(filename)[-1]\n",
    "    spine_id = extract_spine_id(filename)\n",
    "    vertebra_id = extract_vertebra_id(filename)\n",
    "\n",
    "    timestamp_id = filename.replace(spine_id + \"_\" + vertebra_id, \"\")\n",
    "    timestamp_id = timestamp_id.split(\".\")[0]\n",
    "\n",
    "    return timestamp_id\n",
    "\n",
    "\n",
    "def order_files_in_fold(src_filepath, dst_folder, copy=True):\n",
    "    \"\"\"\n",
    "    Given a certain filepath, it copies (or moves it if copy==False) it in the correct folder location\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: console\n",
    "        >> src_filepath = VTU_output_from_SOFA\\spine1_vert1_1_0.vtu\n",
    "        >> dst_folder = DB_Folder\n",
    "        >> patient_id = spine1\n",
    "        >> timestamp_id = _1_0\n",
    "\n",
    "        by running:\n",
    "        >> order_files_in_fold(src_filepath, dst_folder, patient_id, timestamp_id, copy=True)\n",
    "\n",
    "        The file will be copied in DB_Folder/spine_1/ts_1_0/spine1_vert1_1_0.vtu\n",
    "    \"\"\"\n",
    "\n",
    "    patient_id = extract_spine_id(src_filepath)\n",
    "    timestamp_id = extract_timestamp_id(src_filepath)\n",
    "\n",
    "    src_filename = os.path.split(src_filepath)[-1]\n",
    "\n",
    "    # minor correction in naming - the first deformation file misses the timestamp 0\n",
    "    if timestamp_id.count(\"_\") == 1:\n",
    "        timestamp_id = \"_0\" + timestamp_id\n",
    "\n",
    "    dst_folder = os.path.join(dst_folder, patient_id, \"ts\" + timestamp_id)\n",
    "\n",
    "    if not os.path.exists(dst_folder):\n",
    "        os.makedirs(dst_folder)\n",
    "\n",
    "    dst_filepath = os.path.join(dst_folder, src_filename)\n",
    "\n",
    "    if copy:\n",
    "        copy2(src_filepath, dst_filepath)\n",
    "\n",
    "# order_files_in_fold(src_filepath=\"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/obj_files 2\",\n",
    "#                   dst_folder=\"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/obj_files\", copy=False)\n",
    "\n",
    "        \n",
    "def reorder_vtu_files(src_vtu_dir, dst_vtu_dir, copy=True):\n",
    "\n",
    "    \"\"\"\n",
    "    Given a certain filepath containing the .vtu (or any other format) obtained by sofa with the following naming:\n",
    "    spine<spineId>_vert<vertId><timestamp_id>.vtu, it reorders them in the dst_vtu_dir, such that a given file\n",
    "    spine<spineId>_vert<vertId><timestamp_id>.vtu is stored in\n",
    "    dst_vtu_dir/spine<spineId>/<timestamp_id>/spine<spineId>_vert<vertId><timestamp_id>.vtu\n",
    "\n",
    "    Example:\n",
    "        given a src_vtu_dir containing\n",
    "        ['spine1_vert10.vtu', 'spine1_vert1_0.vtu', 'spine1_vert1_1.vtu', 'spine1_vert1_10_0.vtu',\n",
    "        'spine1_vert1_10_1.vtu', ..., 'spine1_vert5_7_1.vtu', 'spine1_vert5_8_0.vtu', 'spine1_vert5_8_1.vtu',\n",
    "        'spine1_vert5_9_0.vtu', 'spine1_vert5_9_1.vtu''spine2_vert10.vtu', 'spine2_vert1_10_0.vtu',\n",
    "        'spine2_vert1_11_0.vtu', 'spine2_vert1_12_0.vtu', 'spine2_vert1_13_0.vtu', ... ]\n",
    "\n",
    "    The function copies (or moves if copy == False) them\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: console\n",
    "            >> src_filepath = VTU_output_from_SOFA\\spine1_vert1_1_0.vtu\n",
    "            >> dst_folder = DB_Folder\n",
    "            >> patient_id = spine1\n",
    "            >> timestamp_id = _1_0\n",
    "\n",
    "    by running:\n",
    "    >> order_files_in_fold(src_filepath, dst_folder, patient_id, timestamp_id, copy=True)\n",
    "\n",
    "    The files are reordered in the dst_vtu_dir as follows:\n",
    "    dst_vtu_dir\\spine1\\ts10\\spine1_vert10.vtu\n",
    "    dst_vtu_dir\\spine1\\ts10\\spine1_vert20.vtu\n",
    "    dst_vtu_dir\\spine1\\ts10\\spine1_vert30.vtu\n",
    "    dst_vtu_dir\\spine1\\ts10\\spine1_vert40.vtu\n",
    "    dst_vtu_dir\\spine1\\ts10\\spine1_vert50.vtu\n",
    "                    ...\n",
    "    dst_vtu_dir\\spine1\\ts10\\spine1_vert1_10_0.vtu\n",
    "    dst_vtu_dir\\spine1\\ts10\\spine1_vert2_10_0.vtu\n",
    "    dst_vtu_dir\\spine1\\ts10\\spine1_vert3_10_0.vtu\n",
    "    dst_vtu_dir\\spine1\\ts10\\spine1_vert4_10_0.vtu\n",
    "    dst_vtu_dir\\spine1\\ts10\\spine1_vert5_10_0.vtu\n",
    "                    ...\n",
    "    dst_vtu_dir\\spine2\\ts10\\spine1_vert1_10_0.vtu\n",
    "    dst_vtu_dir\\spine2\\ts10\\spine1_vert2_10_0.vtu\n",
    "    dst_vtu_dir\\spine2\\ts10\\spine1_vert3_10_0.vtu\n",
    "    dst_vtu_dir\\spine2\\ts10\\spine1_vert4_10_0.vtu\n",
    "    dst_vtu_dir\\spine2\\ts10\\spine1_vert5_10_0.vtu\n",
    "                    ...\n",
    "    \"\"\"\n",
    "\n",
    "    for file in [item for item in os.listdir(src_vtu_dir) if \".vtu\" in item]:\n",
    "        order_files_in_fold(src_filepath=os.path.join(src_vtu_dir, file),\n",
    "                            dst_folder=dst_vtu_dir,\n",
    "                            copy=copy)\n",
    "# reorder_vtu_files(src_vtu_dir=\"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/obj_files 2\",\n",
    "#                   dst_vtu_dir=\"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/obj_files\")\n",
    "reorder_vtu_files(src_vtu_dir=\"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/vtu_files\",\n",
    "                  dst_vtu_dir=\"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/reordered_vtu_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23a42b1",
   "metadata": {},
   "source": [
    "### 2. 2 vtu to txt\n",
    "Generating .txt point cloud files from the Dataset folder containing the vtu files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79dee32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "def vtu2txt(src_vtu_dir, dst_txt_dir):\n",
    "    \"\"\"\n",
    "    Converts a .vtu file in output from sofa to a .txt point cloud file by copying the points coordinates to the\n",
    "    .txt file\n",
    "\n",
    "    :param src_vtu_dir: str: path to the folder containing the .vtu files, ordered according to the reorder_vtu_files\n",
    "    script (e.g. where files are saved according to:\n",
    "     vtu_dir\\spine<spineId>\\ts<timestampId>\\spine<spineId>_vert<vertId>_<timestampId>.vtu)\n",
    "    :param dst_txt_dir: str: path to the folder where the .txt files will be saved, according to the usual folder\n",
    "    structure dst_txt_dir\\spine<spineId>\\ts<timestampId>\\spine<spineId>_vert<vertId>_<timestampId>.txt)\n",
    "    \"\"\"\n",
    "    #check lines which start anything but numbers(in the vtu files the rows \n",
    "    #with numbers are the rows containing the point cloud)\n",
    "    regex = re.compile(\"^ *<|^  +\\d|^\\t<|^\\t\\d|^\\t-|^ +-\")\n",
    "\n",
    "    patient_id_list = [item for item in os.listdir(src_vtu_dir) if \"spine\" in item]\n",
    "\n",
    "    for patient_id in patient_id_list:\n",
    "        timestamp_list = [item for item in os.listdir(os.path.join(src_vtu_dir, patient_id)) if \"ts\" in item]\n",
    "\n",
    "        for timestamp in timestamp_list:\n",
    "            file_list = [item for item in os.listdir(os.path.join(src_vtu_dir, patient_id, timestamp))\n",
    "                         if item.endswith(\".vtu\")]\n",
    "\n",
    "            #write them in a file with same name but ending txt\n",
    "            dst_folder = os.path.join(dst_txt_dir, patient_id, timestamp)\n",
    "            if not os.path.exists(dst_folder):\n",
    "                os.makedirs(dst_folder)\n",
    "\n",
    "            #read all files one by one\n",
    "            for file in file_list:\n",
    "                with open(os.path.join(src_vtu_dir, patient_id, timestamp, file), \"r\") as f:\n",
    "                    lines = f.readlines()\n",
    "                #filter them using the regex above\n",
    "                filtered = [i for i in lines if not regex.match(i)]\n",
    "                dst_filename = os.path.join(dst_folder, file.replace(\".vtu\", \".txt\"))\n",
    "\n",
    "                with open(dst_filename, \"w+\") as f:\n",
    "                    for l in filtered:\n",
    "\n",
    "                        x,y,z = l.split(\" \")\n",
    "                        f.write(\"{0} {1} {2}\\n\".format(float(x)*1e+3,float(y)*1e+3,float(z)*1e+3))\n",
    "\n",
    "vtu2txt(src_vtu_dir = \"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/reordered_vtu_files\",\n",
    "        dst_txt_dir = \"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/txt_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f119d9e5",
   "metadata": {},
   "source": [
    "### 2. 3 vtu to obj\n",
    "Generating .obj meshes from the Dataset folder containing the vtu files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9326fdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import meshio\n",
    "\n",
    "def vtu2obj(src_vtu_dir, dst_obj_dir):\n",
    "    \"\"\"\n",
    "    Converts a .vtu file in output from sofa to a .obj file containing the point cloud mesh.\n",
    "\n",
    "    :param src_vtu_dir: str: path to the folder containing the .vtu files, ordered according to the reorder_vtu_files\n",
    "    script (e.g. where files are saved according to:\n",
    "     vtu_dir\\spine<spineId>\\ts<timestampId>\\spine<spineId>_vert<vertId>_<timestampId>.vtu)\n",
    "    :param dst_obj_dir: str: path to the folder where the .txt files will be saved, according to the usual folder\n",
    "    structure dst_obj_dir\\spine<spineId>\\ts<timestampId>\\spine<spineId>_vert<vertId>_<timestampId>.obj)\n",
    "    \"\"\"\n",
    "\n",
    "    patient_id_list = [item for item in os.listdir(src_vtu_dir) if \"spine\" in item]\n",
    "\n",
    "    for patient_id in patient_id_list:\n",
    "        timestamp_list = [item for item in os.listdir(os.path.join(src_vtu_dir, patient_id)) if \"ts\" in item]\n",
    "\n",
    "        for timestamp in timestamp_list:\n",
    "            file_list = [item for item in os.listdir(os.path.join(src_vtu_dir, patient_id, timestamp))\n",
    "                         if item.endswith(\".vtu\")]\n",
    "\n",
    "            #write them in a file with same name but ending txt\n",
    "            dst_folder = os.path.join(dst_obj_dir, patient_id, timestamp)\n",
    "            if not os.path.exists(dst_folder):\n",
    "                os.makedirs(dst_folder)\n",
    "\n",
    "            for file in file_list:\n",
    "                mesh_vtu = meshio.read(os.path.join(src_vtu_dir, patient_id, timestamp, file))\n",
    "\n",
    "                mesh = meshio.Mesh(\n",
    "                    mesh_vtu.points*1e3,\n",
    "                    mesh_vtu.cells,\n",
    "                    # Optionally provide extra data on points, cells, etc.\n",
    "                    mesh_vtu.point_data,\n",
    "                    # Each item in cell data must match the cells array\n",
    "                    mesh_vtu.cell_data,\n",
    "                    )\n",
    "\n",
    "                dst_filename = os.path.join(dst_folder, file.replace(\".vtu\", \".obj\"))\n",
    "                mesh.write(dst_filename)\n",
    "\n",
    "vtu2obj(src_vtu_dir = \"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/reordered_vtu_files\",\n",
    "        dst_obj_dir = \"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/obj_files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52747bbb",
   "metadata": {},
   "source": [
    "### 2.4 Generating the label maps objects from the .obj files using in ImFusion.\n",
    "To generate the .mhd label maps from the ImFusion we run the imfusion_workspace/create_mhd_spines_from_obj.iws\n",
    "using the batch file that can be generated with the script below.\n",
    "The generated .mhd labelmaps will be saved in the dst_mhd_dir, according to the previously described data structure\n",
    "(i.e. (<dst_mhd_dir>\\spine<spine_id>\\<timestamp_id>.mhd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0aee890c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "def generate_obj2mhd_batch_file(src_obj_dir, dst_mhd_dir, batch_file_path):\n",
    "    \"\"\"\n",
    "    Generates the batch file for the imfusion_workspaces/create_mhd_spines_from_obj.iws imfusion workspace.\n",
    "    Example\n",
    "    .. code-block:: text\n",
    "        INPUT1;INPUT2;INPUT3;INPUT4;INPUT5;OUTPUT\n",
    "        obj_dir\\spine<spineId>\\ts<timestampId>\\spine<spineId>_vert1_<timestampId>.obj;  \\\n",
    "            obj_dir\\spine<spineId>\\ts<timestampId>\\spine<spineId>_vert2_<timestampId>.obj; \\\n",
    "            obj_dir\\spine<spineId>\\ts<timestampId>\\spine<spineId>_vert3_<timestampId>.obj; \\\n",
    "            obj_dir\\spine<spineId>\\ts<timestampId>\\spine<spineId>_vert4_<timestampId>.obj; \\\n",
    "            obj_dir\\spine<spineId>\\ts<timestampId>\\spine<spineId>_vert5_<timestampId>.obj; \\\n",
    "            mhd_dir\\spine<spineId>\\ts<timestampId>.mhd; \\\n",
    "         ...\n",
    "\n",
    "    \"\"\"\n",
    "    patient_id_list = [item for item in os.listdir(src_obj_dir) if \"spine\" in item]\n",
    "\n",
    "    fid = open(batch_file_path, 'w')\n",
    "    fid.write('INPUT1;INPUT2;INPUT3;INPUT4;INPUT5;OUTPUT')\n",
    "    for patient_id in patient_id_list:\n",
    "        timestamp_list = [item for item in os.listdir(os.path.join(src_obj_dir, patient_id)) if \"ts\" in item]\n",
    "\n",
    "        for timestamp in timestamp_list:\n",
    "            if timestamp.endswith(\".imf\"):\n",
    "                continue\n",
    "            print(timestamp)\n",
    "            file_list = [item for item in os.listdir(os.path.join(src_obj_dir, patient_id, timestamp))\n",
    "                         if item.endswith(\".obj\")]\n",
    "\n",
    "            if not os.path.exists(os.path.join(dst_mhd_dir, patient_id)):\n",
    "                os.makedirs(os.path.join(dst_mhd_dir, patient_id))\n",
    "\n",
    "            mhd_filepath = os.path.join(dst_mhd_dir, patient_id, timestamp + \".mhd\")\n",
    "\n",
    "            fid.write(\"\\n\" + \";\".join([os.path.normpath(os.path.join(src_obj_dir, patient_id, timestamp, file))\n",
    "                                 for file in file_list]) + \";\" + mhd_filepath)\n",
    "\n",
    "generate_obj2mhd_batch_file(src_obj_dir=\"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/obj_files\",\n",
    "                            dst_mhd_dir=\"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/mhd_files\",\n",
    "                            batch_file_path=\"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/imfusion_workspaces/obj2mhd.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d885f6",
   "metadata": {},
   "source": [
    "### 2.5 Raycast the generated .mhd labelmap\n",
    "The script reads the labelmaps and generate the ray-casted .mhd file. The file are saved in the save_root, according\n",
    "to the previously described data_structure (<save_root>\\spine<spine_id>\\<timestamp_id>.mhd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9d13e156",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "\n",
    "def ray_cast_slice(image, spine_id=\"\"):\n",
    "    \"\"\"\n",
    "    Generate a ray-casted version of the input slice, in the rows direction (arrows indicating ray-casting direction).\n",
    "\n",
    "    -> __________________________________________________\n",
    "    -> |                                                 |\n",
    "    -> |                                                 |\n",
    "    -> |                                                 |\n",
    "    -> |                                                 |\n",
    "    -> |                                                 |\n",
    "    -> |                                                 |\n",
    "    -> |_________________________________________________|\n",
    "\n",
    "    Only for spine_8, the ray casting is done from right to left instead of left to right\n",
    "    spine_20 and spine_21 are special case where the slice direction is the same as spine_8 but the dimmensions \n",
    "    on which we iterate are different\n",
    "\n",
    "    __________________________________________________\n",
    "    |                                                 | <-\n",
    "    |                                                 | <-\n",
    "    |                                                 | <-\n",
    "    |                                                 | <-\n",
    "    |                                                 | <-\n",
    "    |                                                 | <-\n",
    "    |_________________________________________________| <-\n",
    "    \"\"\"\n",
    "\n",
    "    rays = np.zeros_like(np.squeeze(np.squeeze(image)))\n",
    "    if spine_id == \"spine8\" :\n",
    "\n",
    "        j_range = range(image.shape[0])\n",
    "\n",
    "        for i in range(image.shape[1]):\n",
    "            for j in j_range:\n",
    "                if image[j, i] != 0:\n",
    "                    rays[j, i] = 1\n",
    "                    break\n",
    "    elif spine_id == \"spine20\" or spine_id == \"spine21\" :\n",
    "        j_range = range(image.shape[1]-1, 0, -1)\n",
    "\n",
    "        for i in range(image.shape[0]):\n",
    "            for j in j_range:\n",
    "                if image[i, j] != 0:\n",
    "                    rays[i, j] = 1\n",
    "                    break\n",
    "    else:\n",
    "\n",
    "        j_range = range(image.shape[0]-1, 0, -1)\n",
    "        for i in range(image.shape[1]):\n",
    "            for j in j_range:\n",
    "                if image[j, i] != 0:\n",
    "                    rays[j, i] = 1\n",
    "                    break\n",
    "\n",
    "    return rays\n",
    "\n",
    "def ray_cast_data(data_path, spine_id):\n",
    "    \"\"\"\n",
    "    Ray cast all the data in the spine dataset\n",
    "\n",
    "    :param: data_path: str: The path to the .mhd file\n",
    "    :param: spine_id:str: The spine id (the raycasting direction is selected depending on the spine_id\n",
    "    \"\"\"\n",
    "\n",
    "    assert data_path.endswith(\".mhd\")\n",
    "\n",
    "    img_mhd = sitk.ReadImage(data_path)\n",
    "    im = sitk.GetArrayFromImage(img_mhd)\n",
    "\n",
    "    raycasted = np.zeros_like(im)\n",
    "\n",
    "    \n",
    "    if spine_id in [\"spine1\", \"spine2\", \"spine3\", \"spine4\", \"spine6\", \"spine7\", \"spine10\",\\\n",
    "                   \"spine11\", \"spine12\", \"spine13\", \"spine14\", \"spine16\", \"spine17\", \"spine22\",\\\n",
    "                   \"spine15\", \"spine18\", \"spine19\"]:\n",
    "        for i in range(im.shape[0]):\n",
    "            raycasted[i,...] = ray_cast_slice(im[i,...], spine_id)\n",
    "    elif spine_id in [\"spine5\", \"spine9\", \"spine8\"]:\n",
    "\n",
    "        if spine_id in [\"spine5\", \"spine9\"]:\n",
    "            for i in range(im.shape[2]):\n",
    "                raycasted[...,i] = ray_cast_slice(im[...,i], spine_id)#spine 5, 9\n",
    "        elif spine_id in [\"spine8\"]:\n",
    "            for i in range(im.shape[2]):\n",
    "                raycasted[...,i] = ray_cast_slice(im[...,i], spine_id)#spine 8,\n",
    "    else:\n",
    "\n",
    "        for i in range(im.shape[1]):\n",
    "            raycasted[:,i,:] = ray_cast_slice(im[:,i,:], spine_id)\n",
    "\n",
    "    # Setting the position and orientation of the ray-casted image\n",
    "    raycasted_img = sitk.GetImageFromArray(raycasted)\n",
    "    raycasted_img.SetDirection(img_mhd.GetDirection())\n",
    "    raycasted_img.SetOrigin(img_mhd.GetOrigin())\n",
    "\n",
    "    return raycasted_img\n",
    "\n",
    "\n",
    "def ray_cast_files(data_root, save_root):\n",
    "    \"\"\"\n",
    "    Ray-casts all the files contained in the dataroot directory and saved according to:\n",
    "    <data_root>\\spine<spineId>\\ts<timestampId>.mhd\n",
    "\n",
    "    :param: data_root: str: The path to the data, which must be saved according to:\n",
    "        <data_root>\\spine<spineId>\\ts<timestampId>.mhd\n",
    "    :param: save_root: the path where the raycasted data will be saved, according to:\n",
    "        <save_root>\\spine<spineId>\\raycasted_ts<timestampId>.mhd\n",
    "    \"\"\"\n",
    "\n",
    "    spine_ids =[item for item in os.listdir(data_root) if os.path.isdir(os.path.join(data_root, item)) and\n",
    "                \"spine\" in item]\n",
    "\n",
    "    for spine_id in spine_ids:\n",
    "        spine_folder_path = os.path.join(data_root, spine_id)\n",
    "\n",
    "        save_spine_folder = os.path.join(save_root, spine_id)\n",
    "        if not os.path.exists(save_spine_folder):\n",
    "            os.makedirs(save_spine_folder)\n",
    "\n",
    "        for file in [item for item in os.listdir(spine_folder_path) if item.endswith(\".mhd\")]:\n",
    "\n",
    "            raycasted_img = ray_cast_data(data_path=os.path.join(spine_folder_path, file),\n",
    "                                      spine_id=spine_id)\n",
    "\n",
    "            save_path = os.path.join(save_spine_folder, \"raycasted_\" + file.split(\".\")[0] + \".mhd\")\n",
    "            sitk.WriteImage(raycasted_img,  save_path)\n",
    "\n",
    "\n",
    "ray_cast_files(data_root=\"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/mhd_files\",\n",
    "               save_root=\"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/mhd_files_raycasted\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7adaad",
   "metadata": {},
   "source": [
    "### 2.6. Convert the generated .mhd label maps to .txt point cloud file.\n",
    "mhd to .txt is done using the ImFusion workspace imfusion_workspaces/labelmap2pc.iws.\n",
    "The batch file to be used with the imfusion workspace can be generated with the script below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fc395f47",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def generate_mhd2pc_batch_file(src_labelmaps_dir, dst_pcs_dir, batch_file_path):\n",
    "    \"\"\"\n",
    "    Generate the script to generate the batch file to be used for launching the imfusion_workspaces/labelmap2pc.iws.\n",
    "\n",
    "    :param: src_labelmaps_dir: str: The path to the labelmaps\n",
    "    :param: dst_pcs_dir: str: The directory where the point cloud files will be saved\n",
    "    :param: batch_file_path: str: The path where the (imfusion) .txt batch file will be generated\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: text\n",
    "            INPUTMHD;OUTPUTPC\n",
    "            <src_labelmaps_dir>\\spine<spineId>\\raycasted_ts<timestampId>.mhd;<dst_pcs_dir>\\spine<spineId>\\raycasted_ts<timestampId>.txt\n",
    "                                        ...\n",
    "\n",
    "    \"\"\"\n",
    "    spine_ids = os.listdir(src_labelmaps_dir)\n",
    "\n",
    "    fid = open(batch_file_path, \"w\")\n",
    "\n",
    "    fid.write(\"INPUTMHD;OUTPUTPC\")\n",
    "\n",
    "    for spine_id in spine_ids:\n",
    "        #it appeared in my pc i had to hard code it @Jane\n",
    "        if spine_id == \".DS_Store\":# or spine_id in [\"spine11\", \"spine12\", \"spine13\", \"spine14\", \"spine16\", \"spine17\", \"spine22\",\\\n",
    "                 # \"spine15\", \"spine18\", \"spine19\", \"spine20\",\"spine21\"]:\n",
    "            continue\n",
    "        dst_spine_id_folder = os.path.join(dst_pcs_dir, spine_id)\n",
    "        if not os.path.exists(dst_spine_id_folder):\n",
    "            os.makedirs(dst_spine_id_folder)\n",
    "        \n",
    "        for file in [item for item in os.listdir(os.path.join(src_labelmaps_dir, spine_id)) if \".mhd\" in item]:\n",
    "\n",
    "            input_mhd = os.path.join(src_labelmaps_dir, spine_id, file)\n",
    "            output_pc = os.path.join(dst_pcs_dir, spine_id, file.replace(\".mhd\", \".txt\"))\n",
    "\n",
    "            fid.write(\"\\n\" + input_mhd + \";\" + output_pc)\n",
    "\n",
    "    fid.close()\n",
    "\n",
    "generate_mhd2pc_batch_file(src_labelmaps_dir=\"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/mhd_files\",\n",
    "                           dst_pcs_dir=\"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/txt_files\",\n",
    "                           batch_file_path=\"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/imfusion_workspaces/labelmap2pc.txt\")\n",
    "\n",
    "# todo add imfusion workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb1ae65",
   "metadata": {},
   "source": [
    "# 2.7 Divide ray-casted vertebrae\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b294553",
   "metadata": {},
   "source": [
    "# 2.8 Prepare spine data .txt to .npz (to be used as an input to the network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ae846cfd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spine18\n",
      "spine20\n",
      "spine16\n",
      "spine11\n",
      "spine10\n",
      "spine17\n",
      "spine21\n",
      "spine19\n",
      "spine5\n",
      "spine2\n",
      "spine3\n",
      "spine4\n",
      "spine12\n",
      "spine15\n",
      "spine14\n",
      "spine13\n",
      "spine22\n",
      "spine1\n",
      "spine6\n",
      "spine8\n",
      "spine9\n",
      "spine7\n"
     ]
    }
   ],
   "source": [
    "import visualization_utils as utils\n",
    "from scipy.spatial import KDTree\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "\n",
    "list_files = \"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/\" + \"txtFiles/\"\n",
    "\n",
    "class Point:\n",
    "    def __init__(self, x, y, z, color):\n",
    "        \"\"\"\n",
    "        :param: x: float: x coordinate of the point (in mm)\n",
    "        :param: y: float: y coordinate of the point (in mm)\n",
    "        :param: z: float: z coordinate of the point (in mm)\n",
    "        :param: color: int: integer indicating the color of the point\n",
    "        \"\"\"\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.z = z\n",
    "        self.color = color\n",
    "    \n",
    "    def __str__(self):\n",
    "        return f\"[{self.x}, {self.y}, {self.z}, {self.color}]\"\n",
    "    \n",
    "    def _get_pt_as_array(self):\n",
    "        return np.array([self.x, self.y, self.z])\n",
    "\n",
    "    def get_closest_point_in_cloud(self, pc, filter_by_color=True):\n",
    "\n",
    "        distances = np.array(\n",
    "            [np.linalg.norm(x + y + z) for (x, y, z) in np.abs(pc[:, :3] - self._get_pt_as_array())])\n",
    "\n",
    "        if not filter_by_color:\n",
    "            idx = distances.argmin()\n",
    "            return idx, pc[idx]\n",
    "\n",
    "        if len(np.where(pc[:, 3] == self.color)) == 0:\n",
    "            return None, None\n",
    "\n",
    "        distances[pc[:, 3] != self.color] = np.max(distances) + 1\n",
    "\n",
    "        idx = distances.argmin()\n",
    "\n",
    "        return idx, pc[idx]\n",
    "\n",
    "\n",
    "def indexes2points(idxes_list, point_cloud, color=0):\n",
    "    \"\"\"\n",
    "    Converts a list of indexes or a index to a set of Points, extracting the point coordinates from the input\n",
    "    point_cloud\n",
    "\n",
    "    :param: idxes_list: list(int): list of input indexes\n",
    "    :param: point_cloud: np.ndarray of size [Nx3] or [Nx4]. If the array size is [Nx4], the last dimension is considered\n",
    "        to be the color of the point\n",
    "    :param: color: if the array has size [Nx3], the the color of all the points in the point cloud is set to color\n",
    "        (Default to 0).\n",
    "    :return: a list of Point objects, containing the 3d coordinates and color of the input point cloud at the input\n",
    "        indexes\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: console\n",
    "            >> idxes_list = [1, 3]\n",
    "            >> point_cloud = np.ndarray([ 10, 14, 20, 1\n",
    "                                        [ 10, 30, 20, 4],\n",
    "                                        [ 18, 20, 18, 2],\n",
    "                                        [40, 1, 20, 2])\n",
    "\n",
    "            >> indexes2points(idxes_list, point_cloud)\n",
    "            [Point(x = 10, y = 30, z = 4, color = 1), Point(x = 40, y=1, z = 20, color = 2)]\n",
    "    \"\"\"\n",
    "\n",
    "    if point_cloud.shape[1] > 3:\n",
    "        color = point_cloud[:, 3]\n",
    "    else:\n",
    "        color = np.ones([point_cloud.shape[0],])*color\n",
    "\n",
    "    if isinstance(idxes_list, int) or isinstance(idxes_list, float):\n",
    "        idxes_list = [idxes_list]\n",
    "\n",
    "    constraints_points = []\n",
    "    for item in idxes_list:\n",
    "        if isinstance(item, tuple) or isinstance(item, list):\n",
    "            assert all(isinstance(x, int) for x in item) or all(isinstance(x, float) for x in item)\n",
    "\n",
    "            constraints_points.append(tuple(Point(x=point_cloud[idx, 0],\n",
    "                                                  y=point_cloud[idx, 1],\n",
    "                                                  z=point_cloud[idx, 2],\n",
    "                                                  color=color[idx]) for idx in item))\n",
    "\n",
    "        else:\n",
    "            constraints_points.append(Point(x=point_cloud[item, 0],\n",
    "                                             y=point_cloud[item, 1],\n",
    "                                             z=point_cloud[item, 2],\n",
    "                                             color=color[item]))\n",
    "\n",
    "    if len(constraints_points) == 1:\n",
    "        return constraints_points[0]\n",
    "\n",
    "    return constraints_points\n",
    "\n",
    "\n",
    "def points2indexes(point_list, point_cloud):\n",
    "    \"\"\"\n",
    "    Converts a list of indexes or a points to a set of indexes, corresponding to indexes of the closest points in the\n",
    "    source point cloud.\n",
    "\n",
    "    :param: point_list: list(Point): list of input Point\n",
    "    :param: point_cloud: np.ndarray of size [Nx3]. If the number of columns is higher than 3 (e.g. the input array\n",
    "        has size [Nx4], then only the first 3 columns are considered)\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: console\n",
    "            >> idxes_list = [Point(x = 11, y = 29, z = 3, color = 1), Point(x = 41, y=1, z = 20, color = 2)]\n",
    "            >> point_cloud = np.ndarray([ 10, 14, 20, 1\n",
    "                                        [ 10, 30, 20, 4],\n",
    "                                        [ 18, 20, 18, 2],\n",
    "                                        [40, 1, 20, 2])\n",
    "\n",
    "            >> indexes2points(idxes_list, point_cloud)\n",
    "            [1, 3]\n",
    "    \"\"\"\n",
    "\n",
    "    idxes_list = []\n",
    "\n",
    "    for item in point_list:\n",
    "        if isinstance(item, tuple) or isinstance(item, list):\n",
    "            assert all(isinstance(x, Point) for x in item)\n",
    "            idxes_list.append(tuple(p.get_closest_point_in_cloud(point_cloud)[0] for p in item))\n",
    "\n",
    "        else:\n",
    "            idxes_list.append(item.get_closest_point_in_cloud(point_cloud[0]))\n",
    "\n",
    "    return idxes_list\n",
    "\n",
    "def obtain_indices_raycasted_original_pc(spine_target, r_target):\n",
    "    \"\"\"\n",
    "    Find indices in spine_target w.r.t. r_target such that they are the closest points between the two\n",
    "    point clouds\n",
    "\n",
    "    :param: spine_target: np.ndarray with size [Nx3] with the point cloud for which the closest point indexes are\n",
    "        extracted If the second dimension is higher then 3, only the first 3 dimensions are considered\n",
    "    :param: r_target: np.ndarray with size [Nx3] with the point cloud used to find the closest points in spine_target.\n",
    "        If the second dimension is higher then 3, only the first 3 dimensions are considered\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: console\n",
    "            >> spine_target = np.ndarray([ 10, 14, 20, 1\n",
    "                                [ 10, 30, 20, 4],\n",
    "                                [ 18, 20, 18, 2],\n",
    "                                [40, 1, 20, 2])\n",
    "\n",
    "            >> r_target = np.ndarray([ 18, 21, 18, 2],\n",
    "                                     [40, 1, 20, 3])\n",
    "\n",
    "            >> obtain_indices_raycasted_original_pc(spine_target, r_target)\n",
    "            [2, 3]\n",
    "    \"\"\"\n",
    "    kdtree=KDTree(spine_target[:,:3])\n",
    "    dist,points=kdtree.query(r_target[:,:3],1)\n",
    "\n",
    "    return list(set(points))\n",
    "\n",
    "def create_source_target_with_vertebra_label(source_pc, target_pc, vert):\n",
    "    \"\"\"\n",
    "    source_pc: source point cloud\n",
    "    target_pc: target point cloud\n",
    "    vert: [1-5] for [L1-L5] vertebra respectively\n",
    "    \n",
    "    this function is to create source and target point clouds with label for each vertebra\n",
    "    \"\"\"\n",
    "    \n",
    "    source = np.ones((source_pc.shape[0], source_pc.shape[1]+1))\n",
    "    source[:, :3]=source_pc\n",
    "    source[:, 3] = source[:, 3]*vert\n",
    "    target = np.ones((target_pc.shape[0], target_pc.shape[1]+1))\n",
    "    target[:, :3]=target_pc\n",
    "    target[:, 3]= target[:, 3]*vert\n",
    "    \n",
    "    return source, target\n",
    "\n",
    "def create_source_target_flow_spine(source_pc, target_pc, vert):\n",
    "    \"\"\"\n",
    "    source_pc: source point cloud\n",
    "    target_pc: target point cloud\n",
    "    vert: [1-5] for [L1-L5] vertebra respectively\n",
    "    \n",
    "    this function is to create source and target point clouds with 7D\n",
    "    where the point clouds are centered.\n",
    "    \"\"\"\n",
    "    \n",
    "    source_pc, target_pc = create_source_target_with_vertebra_label(source_pc, target_pc, vert)\n",
    "\n",
    "    centroid_source = centeroidnp(source_pc)\n",
    "    centroid_target = centeroidnp(target_pc)\n",
    "    \n",
    "    source_7d = create_7D(source_pc, centroid_source, centroid_target)\n",
    "    target_7d = create_7D(target_pc, centroid_source, centroid_target)\n",
    "    \n",
    "    flow = target_7d[:,:3]-source_7d[:,:3]\n",
    "    \n",
    "    return source_7d, target_7d, flow\n",
    "\n",
    "\n",
    "def get_lumbar_vertebrae_dict(folder_path):\n",
    "    \"\"\"\n",
    "    Given a timestamp folder, containing the 5 .txt files corresponding to the lumbar vertebrae, the function loads\n",
    "    the vertebra and returns a dict containing the point clouds.\n",
    "    Example, given the folder TestDataOrderingJane\\txt_files\\spine1\\ts_0_0 containing the files (spine1_vert1_0.txt,\n",
    "    spine1_vert2_0.txt, spine1_vert3_0.txt, spine1_vert4_0.txt, spine1_vert5_0.txt), the function returns a dict like\n",
    "    {\"vert1\" : np.array(..), \"vert2\" : np.array(..), \"vert3\" : np.array(..), \"vert4\" : np.array(..),\n",
    "    \"vert5\" : np.array(..)}, where the np.arrays are Nx3 arrays containing the 3D coordinates of the point clouds\n",
    "    of each vertebra\n",
    "\n",
    "    :param folder_path: str: The path to the folder containing the vertebra point clouds .txt files\n",
    "    \"\"\"\n",
    "\n",
    "    vertebra_files = [item for item in os.listdir(folder_path) if \"vert\" in item]\n",
    "\n",
    "    vertebrae_dict = dict()\n",
    "    for vertebra in [\"vert1\", \"vert2\", \"vert3\", \"vert4\", \"vert5\"]:\n",
    "\n",
    "        vert_file = [item for item in vertebra_files if vertebra in item]\n",
    "        assert len(vert_file) == 1\n",
    "\n",
    "        vertebrae_dict[vertebra] = np.loadtxt(os.path.join(folder_path, vert_file[0]))\n",
    "\n",
    "    return vertebrae_dict\n",
    "\n",
    "def load_biomechanical_constraints(spine_folder_path, source_vertebrae_dict):\n",
    "    \"\"\"\n",
    "    Loads the biomechanical constraints and returns them as a list of tuples like\n",
    "    [(c1_1, c1_2), (c2_1, c2_2), ..., (cn_1, cn_2)] where each tuple contains the Point objects\n",
    "     of the \"starting\" point connected to the spring and the index of the \"ending\" point connected to the spring:\n",
    "\n",
    "    ci_1 _/\\/\\/\\/\\_ ci_2\n",
    "\n",
    "    :param: spine_folder_path: str: The path containing the data for a given spine dataset, where the text file\n",
    "        containing the biomechanical constraints is stored\n",
    "    :param: source_vertebrae_dict: The vertebrae dict containing the point cloud corresponding to each vertebra\n",
    "        like:\n",
    "        source_vertebrae_dict = {\"vert1\" : np.array(..), \"vert2\" : np.array(..), \"vert3\" : np.array(..),\n",
    "            \"vert4\" : np.array(..), \"vert5\" : np.array(..)}\n",
    "    \"\"\"\n",
    "\n",
    "    biomechanical_constraints_path = os.path.join(spine_folder_path,\n",
    "                                                  extract_spine_id(spine_folder_path).replace(\"s\", \"S\")\n",
    "                                                  + \"_biomechanical.txt\")\n",
    "\n",
    "    if not os.path.exists(biomechanical_constraints_path):\n",
    "        return []\n",
    "\n",
    "    # The biomechanical constraints are saved in an array on a single row, like:\n",
    "    # idx_c1_1, idx_c1_2, idx_c2_1, idx_c2_2, ..., idx_cn_1, idx_cn_2\n",
    "#     biomechanical_constraints_array = np.squeeze(np.loadtxt(biomechanical_constraints_path))\n",
    "#     biomechanical_constraint_list = []\n",
    "#     for i in range(0, biomechanical_constraints_array.shape[0] - 1, 2):\n",
    "#         biomechanical_constraint_list.append(\n",
    "#             (int(biomechanical_constraints_array[i]), int(biomechanical_constraints_array[i + 1]) ))\n",
    "    \n",
    "    \n",
    "    # The biomechanical constraints are saved in an n x 2 on a rows, like:\n",
    "    # idx_c1_1, idx_c1_2, \n",
    "    # idx_c2_1, idx_c2_2, \n",
    "    # ..., \n",
    "    # idx_cn_1, idx_cn_2\n",
    "    biomechanical_constraints_array = np.loadtxt(biomechanical_constraints_path)\n",
    "    biomechanical_constraints_array = np.array(\n",
    "        [item for sublist in biomechanical_constraints_array for item in sublist])\n",
    "\n",
    "    biomechanical_constraint_list = []\n",
    "    for i in range(0, biomechanical_constraints_array.shape[0] - 1, 2):\n",
    "        biomechanical_constraint_list.append(\n",
    "            (int(biomechanical_constraints_array[i]), int(biomechanical_constraints_array[i + 1]) ))\n",
    "        \n",
    "    ######################\n",
    "    \n",
    "    constraints_points = []\n",
    "    dict_keys = [item for item in source_vertebrae_dict.keys()]\n",
    "\n",
    "    for i in range(0, biomechanical_constraints_array.shape[0] - 1, 2):\n",
    "\n",
    "        vert_name = dict_keys[int(i/2)]\n",
    "        next_vert_name = dict_keys[int(i/2) + 1]\n",
    "        p1 = indexes2points(int(biomechanical_constraints_array[i]),\n",
    "                            point_cloud=source_vertebrae_dict[vert_name],\n",
    "                            color=int(i/2) + 1)\n",
    "\n",
    "        p2 = indexes2points(int(biomechanical_constraints_array[i+1]),\n",
    "                            point_cloud=source_vertebrae_dict[next_vert_name],\n",
    "                            color=int(i/2) + 2)\n",
    "\n",
    "        constraints_points.append((p1, p2))\n",
    "\n",
    "    return constraints_points\n",
    "\n",
    "\n",
    "def preprocess_spine_data(spine_path):\n",
    "    \"\"\"\n",
    "    Preprocess the data for a given spine dataset. Specifically, for the given spine (i.e. for a given spine_id),\n",
    "    it ierates over all the timestamps for that given spine.\n",
    "    The function does the following.\n",
    "    1. It loads the \"ts0\" as the timestamp of the underformed spine, and therefore of the source spine.\n",
    "    2. It loads the biomechanical constraints for the given spine\n",
    "    3. It iterates over all the timestamps different from t0, where the spine is considered to be deformed compared to\n",
    "        t0, and for each timestamp different from ts0:\n",
    "        3.a Computes the flow from the source to the target points, assuming a correspondence\n",
    "            between points at different timestamps\n",
    "        3.b Concatenates all the vertebrae together for both source (ts0) and target (considered timestamp),\n",
    "            indicating the vertebral level in the resulting concatenated point clouds in a 4th column,\n",
    "            where L1 is indicated with 1, L2 with 2, L3 with 3,\n",
    "            L4 with 4, L5 with 5.\n",
    "        3.c. For each given source-deformed spine pair, generate a Data dict with the following keys:\n",
    "            \"spine_id\", \"source_ts_id\", \"target_ts_id\", \"source_pc\", \"target_pc\", \"flow\", \"biomechanical_constraint\"\n",
    "\n",
    "    \"\"\"\n",
    "    spine_id = os.path.split(spine_path)[-1]\n",
    "\n",
    "    # Get the folder containing the data relative to the un-deformed spine (source) and the list of folders\n",
    "    # containing the deformed spine\n",
    "    source_timestamp = \"ts0\"\n",
    "    deformed_timestamps = [item for item in os.listdir(spine_path) if item != source_timestamp and \"ts\" in item]\n",
    "\n",
    "    # Getting the source vertebrae dict, as {\"vert1\" : np.array(..), \"vert2\" : np.array(..),\n",
    "    # \"vert3\" : np.array(..), \"vert4\" : np.array(..), \"vert5\" : np.array(..)}\n",
    "    source_vertebrae = get_lumbar_vertebrae_dict(os.path.join(spine_path, source_timestamp))\n",
    "\n",
    "    # Load the biomechanical constraints for the selected spine. biomechanical_constraints is loaded as a list\n",
    "    # of tuples (Point, Point). biomechanical_constraints = [(Point, Point), (Point, Point), ..., (Point, Point)]\n",
    "    # For a given tuple, the first element is the point from which the spring starts, the second point is the point\n",
    "    # where the spring ends. Note that the biomechanical_constraints contain tuple defining the 3D position of the\n",
    "    # constraints, and not their indexes.\n",
    "    biomechanical_constraints = load_biomechanical_constraints(spine_path, source_vertebrae)\n",
    "\n",
    "    # Iterate over all the deformed versions (folders) of the source spine and generate the data list\n",
    "    data = []\n",
    "    for deformed_timestamp in deformed_timestamps:\n",
    "\n",
    "        # Getting the target vertebrae dict, as {\"vert1\" : np.array(..), \"vert2\" : np.array(..),\n",
    "        # \"vert3\" : np.array(..), \"vert4\" : np.array(..), \"vert5\" : np.array(..)}\n",
    "        deformed_vertebrae = get_lumbar_vertebrae_dict(os.path.join(spine_path, deformed_timestamp))\n",
    "\n",
    "        # Preprocess the point clouds of each given vertebra and then concatenate the vertebrae in a single point cloud\n",
    "        preprocessed_source_vertebrae = []\n",
    "        preprocessed_target_vertebrae = []\n",
    "        for i, vertebra in enumerate([\"vert1\", \"vert2\", \"vert3\", \"vert4\", \"vert5\"]):\n",
    "            preprocessed_source_pc, preprocessed_target_pc = \\\n",
    "                create_source_target_with_vertebra_label(source_pc=source_vertebrae[vertebra],\n",
    "                                                         target_pc=deformed_vertebrae[vertebra],\n",
    "                                                         vert=i + 1)\n",
    "            preprocessed_source_vertebrae.append(preprocessed_source_pc)\n",
    "            preprocessed_target_vertebrae.append(preprocessed_target_pc)\n",
    "\n",
    "        # Concatenating source and target vertebrae into a single spine point cloud\n",
    "        preprocessed_source_spine = np.concatenate(preprocessed_source_vertebrae)\n",
    "        preprocessed_target_spine = np.concatenate(preprocessed_target_vertebrae)\n",
    "\n",
    "        # Append the generated source-target pair to the data list\n",
    "        data.append({\n",
    "            \"spine_id\": spine_id,\n",
    "            \"source_ts_id\": source_timestamp,\n",
    "            \"target_ts_id\": deformed_timestamp,\n",
    "            \"source_pc\": preprocessed_source_spine,\n",
    "            \"target_pc\": preprocessed_target_spine,\n",
    "            \"flow\": preprocessed_target_spine[:, :3] - preprocessed_source_spine[:, :3],\n",
    "            \"biomechanical_constraint\": biomechanical_constraints\n",
    "        })\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def get_ray_casted_data(data, raycasted_txt_path):\n",
    "    # Loading the raycasted point clouds\n",
    "    source_ray_casted_pc = np.loadtxt(os.path.join(raycasted_txt_path, data[\"spine_id\"],\n",
    "                                                   \"raycasted_\" + data[\"source_ts_id\"] + \".txt\"))\n",
    "    target_ray_casted_pc = np.loadtxt(os.path.join(raycasted_txt_path, data[\"spine_id\"],\n",
    "                                                   \"raycasted_\" + data[\"target_ts_id\"] + \".txt\"))\n",
    "\n",
    "    # Getting the flow at the biomechanical_constraints points as it will be needed later\n",
    "    constraint_indexes = points2indexes(point_list=data[\"biomechanical_constraint\"],\n",
    "                                        point_cloud=data[\"source_pc\"])\n",
    "\n",
    "    constraint_points, constraint_flows = [], []\n",
    "    for (p1_idx, p2_idx) in constraint_indexes:\n",
    "        p1_colored, p2_colored = data[\"source_pc\"][p1_idx, :], data[\"source_pc\"][p2_idx, :]\n",
    "        p1_flow, p2_flow = data[\"flow\"][p1_idx, :], data[\"flow\"][p2_idx, :]\n",
    "\n",
    "        constraint_points.append((p1_colored, p2_colored))\n",
    "        constraint_flows.append((p1_flow, p2_flow))\n",
    "\n",
    "    # Getting the indexes of the points in the source data which are closest to the ray_casted source points\n",
    "    source_ray_casted_idxes = obtain_indices_raycasted_original_pc(spine_target=data[\"source_pc\"],\n",
    "                                                                   r_target=source_ray_casted_pc)\n",
    "    data[\"source_pc\"] = data[\"source_pc\"][source_ray_casted_idxes]\n",
    "    data[\"flow\"] = data[\"flow\"][source_ray_casted_idxes]\n",
    "\n",
    "    # Getting the indexes of the points in the target data which are closest to the ray_casted target points\n",
    "    target_ray_casted_idxes = obtain_indices_raycasted_original_pc(spine_target=data[\"target_pc\"],\n",
    "                                                                   r_target=target_ray_casted_pc)\n",
    "    data[\"target_pc\"] = data[\"target_pc\"][target_ray_casted_idxes]\n",
    "\n",
    "    # Adding the biomechanical constraints to the source as they might be not present due to the ray-casting\n",
    "    new_constraints_idx = []\n",
    "    for (p1, p2), (flow1, flow2) in zip(constraint_points, constraint_flows):\n",
    "        data[\"source_pc\"] = np.concatenate((data[\"source_pc\"], np.reshape(p1, [1, 4]), \n",
    "                                            np.reshape(p2, [1, 4])), axis=0)\n",
    "        data[\"flow\"] = np.concatenate((data[\"flow\"], np.reshape(flow1, [1, 3]), \n",
    "                                       np.reshape(flow2, [1, 3])), axis=0)\n",
    "\n",
    "    return data\n",
    "\n",
    "def get_color_code(color_name):\n",
    "    color_code_dict = {\n",
    "        \"dark_green\" : \"0 0.333 0 1\",\n",
    "        \"yellow\": \"1 1 0 1\",\n",
    "        \"default\": \"1 1 0 1\"\n",
    "    }\n",
    "\n",
    "    if color_name in color_code_dict.keys():\n",
    "        return color_code_dict[color_name]\n",
    "\n",
    "    else:\n",
    "        return color_code_dict[\"default\"]\n",
    "\n",
    "def save_for_sanity_check(data, save_dir):\n",
    "    \"\"\"\n",
    "    Saving the generated data in imfusion workspaces at specific location\n",
    "    \"\"\"\n",
    "\n",
    "    source_pc = data[\"source_pc\"][:, :3]\n",
    "    target_pc = data[\"target_pc\"][:, :3]\n",
    "\n",
    "    gt_target_pc = source_pc + data[\"flow\"]\n",
    "\n",
    "    save_folder_path = os.path.join(save_dir, data[\"spine_id\"], data[\"target_ts_id\"])\n",
    "    if not os.path.exists(save_folder_path):\n",
    "        os.makedirs(save_folder_path)\n",
    "\n",
    "    # saving the point clouds\n",
    "    # 1. Saving the full point clouds\n",
    "    np.savetxt(os.path.join(save_folder_path, \"full_source_pc.txt\"), source_pc[:, :3])\n",
    "    np.savetxt(os.path.join(save_folder_path, \"full_target_pc.txt\"), target_pc[:, :3])\n",
    "    np.savetxt(os.path.join(save_folder_path, \"full_gt_pc.txt\"), gt_target_pc[:, :3])\n",
    "\n",
    "    ps_list = [(\"full_source_pc\", os.path.join(save_folder_path, \"full_source_pc.txt\"),\n",
    "                get_color_code(\"dark_green\")),\n",
    "               (\"full_target_pc\", os.path.join(save_folder_path, \"full_target_pc.txt\"), \n",
    "                get_color_code(\"yellow\")),\n",
    "               (\"full_gt_pc\", os.path.join(save_folder_path, \"full_gt_pc.txt\"), \n",
    "                get_color_code(\"yellow\"))]\n",
    "\n",
    "    imf_tree, imf_root = utils.get_empty_imfusion_ws()\n",
    "\n",
    "    for i, (name, path, color) in enumerate(ps_list):\n",
    "\n",
    "        imf_root = utils.add_block_to_xml(imf_root,\n",
    "                                          parent_block_name=\"Annotations\",\n",
    "                                          block_name=\"point_cloud_annotation\",\n",
    "                                          param_dict={\"referenceDataUid\":\"data\" + str(i),\n",
    "                                                      \"name\": str(name),\n",
    "                                                      \"color\": str(color),\n",
    "                                                      \"labelText\":\"some\",\n",
    "                                                      \"pointSize\": \"2\"})\n",
    "\n",
    "        imf_root = utils.add_block_to_xml(imf_root,\n",
    "                                          parent_block_name=\"Algorithms\",\n",
    "                                          block_name=\"load_point_cloud\",\n",
    "                                          param_dict={\"location\": path,\n",
    "                                                      \"outputUids\": \"data\" + str(i)})\n",
    "\n",
    "    # Adding the biomechanical_constraints\n",
    "\n",
    "    for i, (c1, c2) in enumerate(data[\"biomechanical_constraint\"]):\n",
    "\n",
    "        c1_idx, _ = c1.get_closest_point_in_cloud(data[\"source_pc\"], filter_by_color=True)\n",
    "        c2_idx, _ = c2.get_closest_point_in_cloud(data[\"source_pc\"], filter_by_color=True)\n",
    "\n",
    "        p1 = data[\"source_pc\"][c1_idx, :3]\n",
    "        p2 = data[\"source_pc\"][c2_idx, :3]\n",
    "        points = \" \".join([str(item) for item in p1]) + \" \" + \" \".join([str(item) for item in p2])\n",
    "        imf_root = utils.add_block_to_xml(imf_root,\n",
    "                                          parent_block_name=\"Annotations\",\n",
    "                                          block_name=\"segment_annotation\",\n",
    "                                          param_dict={\"name\": \"constraint_\" + str(i+1),\n",
    "                                                      \"points\": points})\n",
    "\n",
    "    utils.write_on_file(imf_tree, os.path.join(save_folder_path, \"imf_ws.iws\"))\n",
    "\n",
    "\n",
    "def generate_npz_files(src_txt_pc_path, dst_npz_path, src_raycasted_pc_path=\"\", ray_casted=False,\n",
    "                       dst_sanity_check_data=\"\"):\n",
    "\n",
    "    if not os.path.exists(dst_npz_path):\n",
    "        os.makedirs(dst_npz_path)\n",
    "\n",
    "    # Iterate over all the patients (spine_id) in the dataset\n",
    "    for spine_id in os.listdir(src_txt_pc_path):\n",
    "        if spine_id == \".DS_Store\": #or spine_id in [\"spine11\", \"spine12\", \"spine13\", \"spine14\", \"spine16\", \"spine17\", \"spine22\",\\\n",
    "                   #\"spine15\", \"spine18\", \"spine19\", \"spine20\",\"spine21\"]:\n",
    "            continue\n",
    "        print(spine_id)\n",
    "            # Getting the dataset for the specific patient id (spine). It is a list of dict like:\n",
    "            # [{\"source_ts_id\": ts0,\n",
    "            #   \"target_ts_id\": ts_19_0,\n",
    "            #   \"source_pc\": np.ndarray([])\n",
    "            #   \"target_pc\": np.ndarray([])\n",
    "            #   \"biomechanical_constraint\": np.ndarray([])}, ...]\n",
    "        spine_data = preprocess_spine_data(os.path.join(src_txt_pc_path, spine_id))\n",
    "\n",
    "        for data in spine_data:\n",
    "            if ray_casted:\n",
    "                data = get_ray_casted_data(data, src_raycasted_pc_path)\n",
    "\n",
    "#             save_for_sanity_check(data, dst_sanity_check_data)\n",
    "\n",
    "            # convert biomechanical_constraint to a 1-d array, putting all the constraint on a single \n",
    "            # row - this needs to be changed in future to be a list of tuple or similar format where it is clear \n",
    "            # which point belongs to the same connecting spring\n",
    "\n",
    "            constraint_indexes = points2indexes(data[\"biomechanical_constraint\"], data[\"source_pc\"])\n",
    "\n",
    "            flattened_constraints = [i for sub in constraint_indexes for i in sub]\n",
    "            np.savez_compressed(file=os.path.join(dst_npz_path,\n",
    "                                                  \"full_\" + spine_id + \"_\" + data[\"target_ts_id\"] + \".npz\"),\n",
    "                                flow=data[\"flow\"],\n",
    "                                pc1=data[\"source_pc\"],\n",
    "                                pc2=data[\"target_pc\"],\n",
    "                                ctsPts=flattened_constraints)\n",
    "\n",
    "\n",
    "generate_npz_files(src_txt_pc_path=\"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/txt_files\",\n",
    "                   dst_npz_path=\"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/npz_data\")\n",
    "#                    ray_casted=True,\n",
    "#                    src_raycasted_pc_path=\"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/txt_files_raycasted\",\n",
    "#                    dst_sanity_check_data=\"/Users/janelameski/Desktop/jane/sofa/SOFAZIPPED/install/bin/sanity_check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3f01bfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aligned_data(source, target, predicted_flow, gt_flow, tre_points=None):\n",
    "    new_aligned_data = []\n",
    "    new_source = np.zeros_like(source)\n",
    "    new_flow = np.zeros_like(gt_flow)\n",
    "    new_source[:,3] = source[:,3]\n",
    "    new_pred_tre = np.zeros((2*5, 4))\n",
    "    tmp_vert = 0\n",
    "    for vertebrae_level in range(1,6):\n",
    "        \n",
    "        vertebrae_idxes = np.argwhere(source[:, 3] == vertebrae_level).flatten()\n",
    "        predicted_deformed_v = source[vertebrae_idxes,0:3] + predicted_flow[vertebrae_idxes]\n",
    "        source_v = source[vertebrae_idxes, ...]\n",
    "        predict_t = metrics.compute_rigid_transform(source_v[..., 0:3], predicted_deformed_v)\n",
    "        #make homogeneous\n",
    "        predict_t[-1] = 1\n",
    "\n",
    "        transposed_s_s = source_v.T\n",
    "        rigidly_deformed_source = np.matmul(predict_t, transposed_s_s)\n",
    "\n",
    "        final_deformed_source_vert = rigidly_deformed_source[0:3,...].T\n",
    "        \n",
    "        gt_deformed = source[vertebrae_idxes, 0:3] + gt_flow[vertebrae_idxes]\n",
    "        \n",
    "        new_source[vertebrae_idxes, :3] = final_deformed_source_vert\n",
    "        new_flow[vertebrae_idxes] = gt_deformed - final_deformed_source_vert\n",
    "        \n",
    "        if tre_points is None:\n",
    "            continue\n",
    "        \n",
    "        vertebrae_target = tre_points[tre_points[:,-1] == vertebrae_level]\n",
    "        #homogeneous\n",
    "        vertebrae_target[:, -1] = 1\n",
    "        \n",
    "        vertebrae_target = vertebrae_target.T\n",
    "        predicted_registered_target = np.matmul(predict_t, vertebrae_target)  # Nx4\n",
    "        print(predicted_registered_target)\n",
    "        new_pred_tre[tmp_vert:vertebrae_level*2] = predicted_registered_target.T\n",
    "        tmp_vert = vertebrae_level*2\n",
    "    if tre_points is not None:\n",
    "        new_pred_tre[:,3] = tre_points[:,3]\n",
    "#         new_pred_tre[vertebrae_level] = predicted_registered_target.T[1,:]\n",
    "\n",
    "        \n",
    "    \n",
    "    if tre_points is None:    \n",
    "        return new_source, target, new_flow, None\n",
    "    else:\n",
    "        return new_source, target, new_flow, new_pred_tre\n",
    "\n",
    "def files_to_folder_lists(path_to_model_output):\n",
    "    \"\"\"\n",
    "    split the names of the files created from the output of the model in\n",
    "    predicted, source, target and ground truth lists\n",
    "    \"\"\"\n",
    "    predicted_paths = []\n",
    "    source_paths = []\n",
    "    target_paths = []\n",
    "    gt_paths = []\n",
    "    for path in os.listdir(path_to_model_output):\n",
    "        if path.startswith(\"predicted\"):\n",
    "            predicted_paths.append(path)\n",
    "        elif path.startswith(\"source\"):\n",
    "            source_paths.append(path)\n",
    "        elif path.startswith(\"target\"):\n",
    "            target_paths.append(path)\n",
    "        elif path.startswith(\"gt\"):\n",
    "            gt_paths.append(path)\n",
    "    \n",
    "    predicted_paths = sorted(predicted_paths)\n",
    "    source_paths = sorted(source_paths)\n",
    "    target_paths = sorted(target_paths)\n",
    "    gt_paths = sorted(gt_paths)\n",
    "    \n",
    "    return predicted_paths, source_paths, target_paths, gt_paths\n",
    "\n",
    "def aligned_to_npz(predicted_paths, source_paths, target_paths, gt_paths, dst_npz_path):\n",
    "    \"\"\"kjh\"\"\"\n",
    "    for i in range(len(predicted_paths)):\n",
    "        source, target, flow, tre = aligned_data(np.loadtxt(dst_npz_path+source_paths[i])\n",
    "        ,np.loadtxt(dst_npz_path+target_paths[i])\n",
    "        ,np.loadtxt(dst_npz_path+predicted_paths[i])[:,:3] - \\\n",
    "        np.loadtxt(dst_npz_path+source_paths[i])[:,:3]\n",
    "        ,np.loadtxt(dst_npz_path+gt_paths[i])[:,:3] - \\\n",
    "        np.loadtxt(dst_npz_path+source_paths[i])[:,:3])\n",
    "        \n",
    "        if gt_paths[i][gt_paths[i].find(\"spine\") + 6].isdigit()==True:\n",
    "            \n",
    "#             print(os.path.join(dst_npz_path,\n",
    "#                                 \"aligned_\" + gt_paths[i][gt_paths[i].find(\"spine\"):gt_paths[i].find(\"spine\") + 7] + \\\n",
    "#                                 \"_\" + gt_paths[i][gt_paths[i].find(\"ts\"):-4]+ \".npz\"))\n",
    "            np.savez_compressed(file=os.path.join(dst_npz_path,\n",
    "                                \"aligned_raycasted_\" + gt_paths[i][gt_paths[i].find(\"spine\"):gt_paths[i].find(\"spine\") + 7] + \\\n",
    "                                \"_\" + gt_paths[i][gt_paths[i].find(\"ts\"):-4]+ \".npz\"),\n",
    "                                    flow=flow,\n",
    "                                    pc1=source,\n",
    "                                    pc2=target,\n",
    "                                    ctsPts=[i for i in range(4095 - 8)])\n",
    "            if tre is None:\n",
    "                continue\n",
    "            np.savetxt(os.path.join(dst_npz_path, \"aligned_\"+ gt_paths[i][gt_paths[i].find(\"spine\"):gt_paths[i].find(\"spine\") + 7] + \\\n",
    "                                   \"facet_target.txt\"), tre)\n",
    "        else:\n",
    "#             print(os.path.join(dst_npz_path,\n",
    "#                             \"aligned_\" + gt_paths[i][gt_paths[i].find(\"spine\"):gt_paths[i].find(\"spine\") + 6] + \\\n",
    "#                             \"_\" + gt_paths[i][gt_paths[i].find(\"ts\"):-4]+ \".npz\"))\n",
    "            np.savez_compressed(file=os.path.join(dst_npz_path,\n",
    "                                  \"aligned_raycasted_\" + gt_paths[i][gt_paths[i].find(\"spine\"):gt_paths[i].find(\"spine\") + 6] + \\\n",
    "                                  \"_\" + gt_paths[i][gt_paths[i].find(\"ts\"):-4]+ \".npz\"),\n",
    "                                    flow=flow,\n",
    "                                    pc1=source,\n",
    "                                    pc2=target,\n",
    "                                    ctsPts=[i for i in range(4095 - 8)])\n",
    "            if tre is None:\n",
    "                continue\n",
    "            np.savetxt(os.path.join(dst_npz_path, \"aligned_\"+ gt_paths[i][gt_paths[i].find(\"spine\"):gt_paths[i].find(\"spine\") + 7] + \\\n",
    "                                   \"facet_target.txt\"), tre)\n",
    "            \n",
    "# predicted_paths, source_paths, target_paths, gt_paths = files_to_folder_lists(\"/Users/janelameski/PycharmProjects/pythonProject/thesis/Results/flownet3d/test_result/\")    \n",
    "# aligned_to_npz(predicted_paths, source_paths, target_paths, gt_paths,\"/Users/janelameski/PycharmProjects/pythonProject/thesis/Results/flownet3d/test_result/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1db0ca25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -11.38443584   13.57171542]\n",
      " [  99.12348151  100.0757703 ]\n",
      " [-346.63540389 -346.40615103]\n",
      " [-258.00434113 -231.72973251]]\n",
      "[[  21.42998978   -5.45919497]\n",
      " [  92.79521938   94.12108146]\n",
      " [-375.75309139 -377.30108662]\n",
      " [-264.29425907 -291.61031342]]\n",
      "[[  30.81651566   -1.2299494 ]\n",
      " [  94.76247103   93.54808014]\n",
      " [-403.94203584 -406.08096277]\n",
      " [-284.49339485 -320.10886383]]\n",
      "[[  30.58662205   -5.31760195]\n",
      " [  97.50045235   97.13990955]\n",
      " [-428.40452188 -428.23351957]\n",
      " [-303.67665482 -339.9325304 ]]\n",
      "[[  35.41309722   -7.97284737]\n",
      " [ 104.91855186  104.36747592]\n",
      " [-442.98484021 -444.02889946]\n",
      " [-308.06954384 -352.92299557]]\n",
      "[[ -11.38443584   99.12348151 -346.63540389    1.        ]\n",
      " [  13.57171542  100.0757703  -346.40615103    1.        ]\n",
      " [  21.42998978   92.79521938 -375.75309139    2.        ]\n",
      " [  -5.45919497   94.12108146 -377.30108662    2.        ]\n",
      " [  30.81651566   94.76247103 -403.94203584    3.        ]\n",
      " [  -1.2299494    93.54808014 -406.08096277    3.        ]\n",
      " [  30.58662205   97.50045235 -428.40452188    4.        ]\n",
      " [  -5.31760195   97.13990955 -428.23351957    4.        ]\n",
      " [  35.41309722  104.91855186 -442.98484021    5.        ]\n",
      " [  -7.97284737  104.36747592 -444.02889946    5.        ]]\n"
     ]
    }
   ],
   "source": [
    "dst_npz_path = \"/Users/janelameski/PycharmProjects/pythonProject/thesis/Results/flownet3d/test_result/\" \n",
    "source_paths = \"source_raycasted_spine22_ts_10_0.txt\"\n",
    "target_paths = \"target_raycasted_spine22_ts_10_0.txt\"\n",
    "predicted_paths = \"predicted_raycasted_spine22_ts_10_0.txt\"\n",
    "gt_paths = \"gt_raycasted_spine22_ts_10_0.txt\"\n",
    "\n",
    "tre_path = \"spine22_facet_targets.txt\"\n",
    "source, target, flow, tre= aligned_data(np.loadtxt(dst_npz_path+source_paths)\n",
    "        ,np.loadtxt(dst_npz_path+target_paths)\n",
    "        ,np.loadtxt(dst_npz_path+predicted_paths)[:,:3] - \\\n",
    "        np.loadtxt(dst_npz_path+source_paths)[:,:3]\n",
    "        ,np.loadtxt(dst_npz_path+gt_paths)[:,:3] - \\\n",
    "        np.loadtxt(dst_npz_path+source_paths)[:,:3], np.loadtxt(dst_npz_path+tre_path))\n",
    "a = np.loadtxt(dst_npz_path + tre_path)\n",
    "print(tre)\n",
    "np.savetxt(dst_npz_path + \"tre_aligned.txt\", tre)\n",
    "data = np.load(\"/Users/janelameski/PycharmProjects/pythonProject/thesis/Results/flownet3d/test_result/aligned_raycasted_spine22_ts_10_0.npz\")\n",
    "\n",
    "np.savetxt(\"/Users/janelameski/PycharmProjects/pythonProject/thesis/Results/flownet3d/test_result/test_npz_source.txt\",data[\"pc1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ca3bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
